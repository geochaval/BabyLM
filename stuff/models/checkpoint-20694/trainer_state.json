{
  "best_metric": 12.872751235961914,
  "best_model_checkpoint": "models/Baby-Llama-58M-2/checkpoint-20694",
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 20694,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0057987822557262975,
      "grad_norm": 86.4579849243164,
      "learning_rate": 2.6999999999999996e-05,
      "loss": 380.6026,
      "step": 20
    },
    {
      "epoch": 0.011597564511452595,
      "grad_norm": 54.305965423583984,
      "learning_rate": 5.6999999999999996e-05,
      "loss": 353.3877,
      "step": 40
    },
    {
      "epoch": 0.017396346767178893,
      "grad_norm": 44.54051971435547,
      "learning_rate": 8.699999999999999e-05,
      "loss": 316.7454,
      "step": 60
    },
    {
      "epoch": 0.02319512902290519,
      "grad_norm": 41.35471725463867,
      "learning_rate": 0.000117,
      "loss": 281.8033,
      "step": 80
    },
    {
      "epoch": 0.028993911278631487,
      "grad_norm": 21.857421875,
      "learning_rate": 0.000147,
      "loss": 263.5895,
      "step": 100
    },
    {
      "epoch": 0.03479269353435779,
      "grad_norm": 14.434086799621582,
      "learning_rate": 0.00017699999999999997,
      "loss": 254.9243,
      "step": 120
    },
    {
      "epoch": 0.04059147579008408,
      "grad_norm": 34.016822814941406,
      "learning_rate": 0.00020699999999999996,
      "loss": 249.3967,
      "step": 140
    },
    {
      "epoch": 0.04639025804581038,
      "grad_norm": 48.35105514526367,
      "learning_rate": 0.000237,
      "loss": 224.8297,
      "step": 160
    },
    {
      "epoch": 0.05218904030153668,
      "grad_norm": 50.445762634277344,
      "learning_rate": 0.000267,
      "loss": 204.101,
      "step": 180
    },
    {
      "epoch": 0.05798782255726297,
      "grad_norm": 40.176334381103516,
      "learning_rate": 0.00029699999999999996,
      "loss": 193.5961,
      "step": 200
    },
    {
      "epoch": 0.06378660481298927,
      "grad_norm": 65.51127624511719,
      "learning_rate": 0.00029999942897873774,
      "loss": 184.5778,
      "step": 220
    },
    {
      "epoch": 0.06958538706871557,
      "grad_norm": 68.34908294677734,
      "learning_rate": 0.00029999745508365976,
      "loss": 173.6527,
      "step": 240
    },
    {
      "epoch": 0.07538416932444186,
      "grad_norm": 53.219085693359375,
      "learning_rate": 0.00029999407128367004,
      "loss": 167.7201,
      "step": 260
    },
    {
      "epoch": 0.08118295158016817,
      "grad_norm": 42.27157974243164,
      "learning_rate": 0.0002999892776105747,
      "loss": 158.9157,
      "step": 280
    },
    {
      "epoch": 0.08698173383589446,
      "grad_norm": 62.02976989746094,
      "learning_rate": 0.000299983074109432,
      "loss": 153.0787,
      "step": 300
    },
    {
      "epoch": 0.09278051609162076,
      "grad_norm": 45.236595153808594,
      "learning_rate": 0.00029997546083855196,
      "loss": 150.4454,
      "step": 320
    },
    {
      "epoch": 0.09857929834734706,
      "grad_norm": 38.25385665893555,
      "learning_rate": 0.00029996643786949563,
      "loss": 144.3516,
      "step": 340
    },
    {
      "epoch": 0.10437808060307335,
      "grad_norm": 50.19677734375,
      "learning_rate": 0.00029995600528707475,
      "loss": 139.6213,
      "step": 360
    },
    {
      "epoch": 0.11017686285879966,
      "grad_norm": 53.851707458496094,
      "learning_rate": 0.0002999441631893505,
      "loss": 135.0513,
      "step": 380
    },
    {
      "epoch": 0.11597564511452595,
      "grad_norm": 29.150409698486328,
      "learning_rate": 0.00029993091168763306,
      "loss": 131.7541,
      "step": 400
    },
    {
      "epoch": 0.12177442737025225,
      "grad_norm": 38.82897186279297,
      "learning_rate": 0.0002999162509064803,
      "loss": 129.9404,
      "step": 420
    },
    {
      "epoch": 0.12757320962597854,
      "grad_norm": 37.24177932739258,
      "learning_rate": 0.0002999001809836965,
      "loss": 125.4835,
      "step": 440
    },
    {
      "epoch": 0.13337199188170484,
      "grad_norm": 33.94219207763672,
      "learning_rate": 0.0002998827020703314,
      "loss": 122.4425,
      "step": 460
    },
    {
      "epoch": 0.13917077413743115,
      "grad_norm": 42.20863342285156,
      "learning_rate": 0.00029986381433067845,
      "loss": 120.0422,
      "step": 480
    },
    {
      "epoch": 0.14496955639315745,
      "grad_norm": 41.029884338378906,
      "learning_rate": 0.0002998435179422735,
      "loss": 116.7138,
      "step": 500
    },
    {
      "epoch": 0.15076833864888373,
      "grad_norm": 36.82770538330078,
      "learning_rate": 0.0002998218130958929,
      "loss": 114.3061,
      "step": 520
    },
    {
      "epoch": 0.15656712090461003,
      "grad_norm": 48.01751708984375,
      "learning_rate": 0.0002997986999955519,
      "loss": 110.8398,
      "step": 540
    },
    {
      "epoch": 0.16236590316033633,
      "grad_norm": 36.40361022949219,
      "learning_rate": 0.0002997741788585027,
      "loss": 108.1002,
      "step": 560
    },
    {
      "epoch": 0.16816468541606264,
      "grad_norm": 42.98246383666992,
      "learning_rate": 0.0002997482499152324,
      "loss": 107.5433,
      "step": 580
    },
    {
      "epoch": 0.1739634676717889,
      "grad_norm": 39.52524185180664,
      "learning_rate": 0.00029972091340946076,
      "loss": 104.576,
      "step": 600
    },
    {
      "epoch": 0.17976224992751522,
      "grad_norm": 44.404571533203125,
      "learning_rate": 0.0002996921695981379,
      "loss": 101.6116,
      "step": 620
    },
    {
      "epoch": 0.18556103218324152,
      "grad_norm": 34.503177642822266,
      "learning_rate": 0.00029966201875144204,
      "loss": 101.1744,
      "step": 640
    },
    {
      "epoch": 0.19135981443896782,
      "grad_norm": 37.89596939086914,
      "learning_rate": 0.00029963046115277683,
      "loss": 98.3702,
      "step": 660
    },
    {
      "epoch": 0.19715859669469413,
      "grad_norm": 29.348237991333008,
      "learning_rate": 0.00029959749709876875,
      "loss": 97.91,
      "step": 680
    },
    {
      "epoch": 0.2029573789504204,
      "grad_norm": 35.70552062988281,
      "learning_rate": 0.0002995631268992643,
      "loss": 95.5618,
      "step": 700
    },
    {
      "epoch": 0.2087561612061467,
      "grad_norm": 40.11915588378906,
      "learning_rate": 0.0002995273508773269,
      "loss": 93.1874,
      "step": 720
    },
    {
      "epoch": 0.214554943461873,
      "grad_norm": 33.145843505859375,
      "learning_rate": 0.0002994901693692343,
      "loss": 91.4801,
      "step": 740
    },
    {
      "epoch": 0.2203537257175993,
      "grad_norm": 36.934085845947266,
      "learning_rate": 0.00029945158272447507,
      "loss": 89.2882,
      "step": 760
    },
    {
      "epoch": 0.2261525079733256,
      "grad_norm": 41.77031326293945,
      "learning_rate": 0.0002994115913057453,
      "loss": 87.7726,
      "step": 780
    },
    {
      "epoch": 0.2319512902290519,
      "grad_norm": 31.848451614379883,
      "learning_rate": 0.0002993701954889453,
      "loss": 87.3427,
      "step": 800
    },
    {
      "epoch": 0.2377500724847782,
      "grad_norm": 42.84062957763672,
      "learning_rate": 0.0002993273956631762,
      "loss": 85.3433,
      "step": 820
    },
    {
      "epoch": 0.2435488547405045,
      "grad_norm": 37.83853530883789,
      "learning_rate": 0.00029928319223073606,
      "loss": 83.8328,
      "step": 840
    },
    {
      "epoch": 0.2493476369962308,
      "grad_norm": 36.50251770019531,
      "learning_rate": 0.0002992375856071161,
      "loss": 82.6271,
      "step": 860
    },
    {
      "epoch": 0.2551464192519571,
      "grad_norm": 32.636592864990234,
      "learning_rate": 0.00029919057622099704,
      "loss": 81.2919,
      "step": 880
    },
    {
      "epoch": 0.2609452015076834,
      "grad_norm": 29.192338943481445,
      "learning_rate": 0.0002991421645142447,
      "loss": 78.9514,
      "step": 900
    },
    {
      "epoch": 0.2667439837634097,
      "grad_norm": 30.80743408203125,
      "learning_rate": 0.00029909235094190617,
      "loss": 77.5595,
      "step": 920
    },
    {
      "epoch": 0.27254276601913596,
      "grad_norm": 31.871723175048828,
      "learning_rate": 0.00029904113597220544,
      "loss": 77.6118,
      "step": 940
    },
    {
      "epoch": 0.2783415482748623,
      "grad_norm": 28.328662872314453,
      "learning_rate": 0.00029898852008653897,
      "loss": 76.337,
      "step": 960
    },
    {
      "epoch": 0.28414033053058857,
      "grad_norm": 29.444581985473633,
      "learning_rate": 0.00029893450377947104,
      "loss": 74.4774,
      "step": 980
    },
    {
      "epoch": 0.2899391127863149,
      "grad_norm": 31.441205978393555,
      "learning_rate": 0.0002988790875587293,
      "loss": 73.7077,
      "step": 1000
    },
    {
      "epoch": 0.2957378950420412,
      "grad_norm": 28.152650833129883,
      "learning_rate": 0.0002988222719452,
      "loss": 71.0725,
      "step": 1020
    },
    {
      "epoch": 0.30153667729776745,
      "grad_norm": 28.63273048400879,
      "learning_rate": 0.00029876405747292293,
      "loss": 70.2388,
      "step": 1040
    },
    {
      "epoch": 0.3073354595534938,
      "grad_norm": 29.341073989868164,
      "learning_rate": 0.0002987044446890865,
      "loss": 70.5377,
      "step": 1060
    },
    {
      "epoch": 0.31313424180922006,
      "grad_norm": 26.940013885498047,
      "learning_rate": 0.0002986434341540226,
      "loss": 68.3317,
      "step": 1080
    },
    {
      "epoch": 0.31893302406494634,
      "grad_norm": 29.8262939453125,
      "learning_rate": 0.0002985810264412013,
      "loss": 67.0214,
      "step": 1100
    },
    {
      "epoch": 0.32473180632067267,
      "grad_norm": 26.885074615478516,
      "learning_rate": 0.0002985172221372255,
      "loss": 65.9517,
      "step": 1120
    },
    {
      "epoch": 0.33053058857639894,
      "grad_norm": 30.9658260345459,
      "learning_rate": 0.0002984520218418253,
      "loss": 65.4609,
      "step": 1140
    },
    {
      "epoch": 0.3363293708321253,
      "grad_norm": 27.585481643676758,
      "learning_rate": 0.0002983854261678527,
      "loss": 64.3339,
      "step": 1160
    },
    {
      "epoch": 0.34212815308785155,
      "grad_norm": 27.89508056640625,
      "learning_rate": 0.0002983174357412754,
      "loss": 63.5552,
      "step": 1180
    },
    {
      "epoch": 0.3479269353435778,
      "grad_norm": 28.461837768554688,
      "learning_rate": 0.00029824805120117105,
      "loss": 61.7283,
      "step": 1200
    },
    {
      "epoch": 0.35372571759930416,
      "grad_norm": 29.883060455322266,
      "learning_rate": 0.00029817727319972153,
      "loss": 61.815,
      "step": 1220
    },
    {
      "epoch": 0.35952449985503043,
      "grad_norm": 29.89190673828125,
      "learning_rate": 0.0002981051024022064,
      "loss": 60.6305,
      "step": 1240
    },
    {
      "epoch": 0.36532328211075676,
      "grad_norm": 26.78911590576172,
      "learning_rate": 0.0002980315394869969,
      "loss": 59.7091,
      "step": 1260
    },
    {
      "epoch": 0.37112206436648304,
      "grad_norm": 29.62561798095703,
      "learning_rate": 0.00029795658514554957,
      "loss": 59.0284,
      "step": 1280
    },
    {
      "epoch": 0.3769208466222093,
      "grad_norm": 25.15001106262207,
      "learning_rate": 0.0002978802400823997,
      "loss": 58.2111,
      "step": 1300
    },
    {
      "epoch": 0.38271962887793565,
      "grad_norm": 26.142440795898438,
      "learning_rate": 0.0002978025050151546,
      "loss": 57.7132,
      "step": 1320
    },
    {
      "epoch": 0.3885184111336619,
      "grad_norm": 26.26645278930664,
      "learning_rate": 0.000297723380674487,
      "loss": 56.8515,
      "step": 1340
    },
    {
      "epoch": 0.39431719338938825,
      "grad_norm": 28.861032485961914,
      "learning_rate": 0.0002976428678041282,
      "loss": 56.5328,
      "step": 1360
    },
    {
      "epoch": 0.40011597564511453,
      "grad_norm": 26.471656799316406,
      "learning_rate": 0.000297560967160861,
      "loss": 55.0597,
      "step": 1380
    },
    {
      "epoch": 0.4059147579008408,
      "grad_norm": 23.85090446472168,
      "learning_rate": 0.00029747767951451257,
      "loss": 54.012,
      "step": 1400
    },
    {
      "epoch": 0.41171354015656714,
      "grad_norm": 24.017475128173828,
      "learning_rate": 0.00029739300564794724,
      "loss": 53.8167,
      "step": 1420
    },
    {
      "epoch": 0.4175123224122934,
      "grad_norm": 30.191587448120117,
      "learning_rate": 0.00029730694635705923,
      "loss": 52.4899,
      "step": 1440
    },
    {
      "epoch": 0.4233111046680197,
      "grad_norm": 25.61383056640625,
      "learning_rate": 0.00029721950245076506,
      "loss": 52.8326,
      "step": 1460
    },
    {
      "epoch": 0.429109886923746,
      "grad_norm": 25.752727508544922,
      "learning_rate": 0.00029713067475099585,
      "loss": 51.5214,
      "step": 1480
    },
    {
      "epoch": 0.4349086691794723,
      "grad_norm": 25.88865089416504,
      "learning_rate": 0.00029704046409269,
      "loss": 51.8015,
      "step": 1500
    },
    {
      "epoch": 0.4407074514351986,
      "grad_norm": 25.186702728271484,
      "learning_rate": 0.0002969488713237847,
      "loss": 50.4198,
      "step": 1520
    },
    {
      "epoch": 0.4465062336909249,
      "grad_norm": 28.19459342956543,
      "learning_rate": 0.0002968558973052087,
      "loss": 49.7672,
      "step": 1540
    },
    {
      "epoch": 0.4523050159466512,
      "grad_norm": 22.10373306274414,
      "learning_rate": 0.0002967615429108736,
      "loss": 48.5663,
      "step": 1560
    },
    {
      "epoch": 0.4581037982023775,
      "grad_norm": 24.211139678955078,
      "learning_rate": 0.0002966658090276659,
      "loss": 48.1844,
      "step": 1580
    },
    {
      "epoch": 0.4639025804581038,
      "grad_norm": 28.519039154052734,
      "learning_rate": 0.00029656869655543875,
      "loss": 48.7064,
      "step": 1600
    },
    {
      "epoch": 0.4697013627138301,
      "grad_norm": 26.13262176513672,
      "learning_rate": 0.0002964702064070032,
      "loss": 48.0291,
      "step": 1620
    },
    {
      "epoch": 0.4755001449695564,
      "grad_norm": 25.32807159423828,
      "learning_rate": 0.00029637033950812003,
      "loss": 47.2125,
      "step": 1640
    },
    {
      "epoch": 0.48129892722528267,
      "grad_norm": 23.675092697143555,
      "learning_rate": 0.0002962690967974907,
      "loss": 46.6656,
      "step": 1660
    },
    {
      "epoch": 0.487097709481009,
      "grad_norm": 24.74079704284668,
      "learning_rate": 0.0002961664792267486,
      "loss": 46.2568,
      "step": 1680
    },
    {
      "epoch": 0.4928964917367353,
      "grad_norm": 24.21219825744629,
      "learning_rate": 0.00029606248776045023,
      "loss": 45.2949,
      "step": 1700
    },
    {
      "epoch": 0.4986952739924616,
      "grad_norm": 22.804847717285156,
      "learning_rate": 0.0002959571233760661,
      "loss": 45.0557,
      "step": 1720
    },
    {
      "epoch": 0.5044940562481879,
      "grad_norm": 21.974849700927734,
      "learning_rate": 0.0002958503870639713,
      "loss": 44.6908,
      "step": 1740
    },
    {
      "epoch": 0.5102928385039142,
      "grad_norm": 24.104751586914062,
      "learning_rate": 0.00029574227982743666,
      "loss": 44.0012,
      "step": 1760
    },
    {
      "epoch": 0.5160916207596404,
      "grad_norm": 25.076889038085938,
      "learning_rate": 0.00029563280268261877,
      "loss": 43.7669,
      "step": 1780
    },
    {
      "epoch": 0.5218904030153668,
      "grad_norm": 23.780624389648438,
      "learning_rate": 0.00029552195665855097,
      "loss": 43.1717,
      "step": 1800
    },
    {
      "epoch": 0.5276891852710931,
      "grad_norm": 24.50978660583496,
      "learning_rate": 0.00029540974279713327,
      "loss": 42.9628,
      "step": 1820
    },
    {
      "epoch": 0.5334879675268194,
      "grad_norm": 27.931318283081055,
      "learning_rate": 0.0002952961621531227,
      "loss": 43.1767,
      "step": 1840
    },
    {
      "epoch": 0.5392867497825456,
      "grad_norm": 20.873859405517578,
      "learning_rate": 0.00029518121579412345,
      "loss": 42.068,
      "step": 1860
    },
    {
      "epoch": 0.5450855320382719,
      "grad_norm": 24.88645362854004,
      "learning_rate": 0.00029506490480057674,
      "loss": 42.4585,
      "step": 1880
    },
    {
      "epoch": 0.5508843142939983,
      "grad_norm": 22.633024215698242,
      "learning_rate": 0.0002949472302657507,
      "loss": 41.3635,
      "step": 1900
    },
    {
      "epoch": 0.5566830965497246,
      "grad_norm": 22.067001342773438,
      "learning_rate": 0.0002948281932957302,
      "loss": 40.6363,
      "step": 1920
    },
    {
      "epoch": 0.5624818788054509,
      "grad_norm": 23.163965225219727,
      "learning_rate": 0.0002947077950094062,
      "loss": 40.9984,
      "step": 1940
    },
    {
      "epoch": 0.5682806610611771,
      "grad_norm": 23.34607696533203,
      "learning_rate": 0.0002945860365384655,
      "loss": 41.0533,
      "step": 1960
    },
    {
      "epoch": 0.5740794433169034,
      "grad_norm": 22.764232635498047,
      "learning_rate": 0.00029446291902738,
      "loss": 39.8099,
      "step": 1980
    },
    {
      "epoch": 0.5798782255726298,
      "grad_norm": 21.000932693481445,
      "learning_rate": 0.00029433844363339586,
      "loss": 39.8742,
      "step": 2000
    },
    {
      "epoch": 0.5856770078283561,
      "grad_norm": 20.97809410095215,
      "learning_rate": 0.00029421261152652275,
      "loss": 39.7247,
      "step": 2020
    },
    {
      "epoch": 0.5914757900840824,
      "grad_norm": 21.564130783081055,
      "learning_rate": 0.0002940854238895227,
      "loss": 38.9918,
      "step": 2040
    },
    {
      "epoch": 0.5972745723398086,
      "grad_norm": 24.510385513305664,
      "learning_rate": 0.0002939568819178992,
      "loss": 38.3842,
      "step": 2060
    },
    {
      "epoch": 0.6030733545955349,
      "grad_norm": 22.452672958374023,
      "learning_rate": 0.00029382698681988586,
      "loss": 38.5753,
      "step": 2080
    },
    {
      "epoch": 0.6088721368512612,
      "grad_norm": 21.037399291992188,
      "learning_rate": 0.00029369573981643485,
      "loss": 38.5576,
      "step": 2100
    },
    {
      "epoch": 0.6146709191069876,
      "grad_norm": 20.75720977783203,
      "learning_rate": 0.0002935631421412058,
      "loss": 38.1606,
      "step": 2120
    },
    {
      "epoch": 0.6204697013627138,
      "grad_norm": 19.907739639282227,
      "learning_rate": 0.00029342919504055396,
      "loss": 37.7622,
      "step": 2140
    },
    {
      "epoch": 0.6262684836184401,
      "grad_norm": 19.288904190063477,
      "learning_rate": 0.00029329389977351847,
      "loss": 37.4268,
      "step": 2160
    },
    {
      "epoch": 0.6320672658741664,
      "grad_norm": 21.44049072265625,
      "learning_rate": 0.00029315725761181073,
      "loss": 37.2583,
      "step": 2180
    },
    {
      "epoch": 0.6378660481298927,
      "grad_norm": 21.779830932617188,
      "learning_rate": 0.0002930192698398022,
      "loss": 37.0137,
      "step": 2200
    },
    {
      "epoch": 0.6436648303856191,
      "grad_norm": 19.853317260742188,
      "learning_rate": 0.0002928799377545124,
      "loss": 36.6166,
      "step": 2220
    },
    {
      "epoch": 0.6494636126413453,
      "grad_norm": 23.102102279663086,
      "learning_rate": 0.00029273926266559693,
      "loss": 36.6908,
      "step": 2240
    },
    {
      "epoch": 0.6552623948970716,
      "grad_norm": 22.34217643737793,
      "learning_rate": 0.0002925972458953348,
      "loss": 36.3596,
      "step": 2260
    },
    {
      "epoch": 0.6610611771527979,
      "grad_norm": 20.138002395629883,
      "learning_rate": 0.0002924538887786163,
      "loss": 35.753,
      "step": 2280
    },
    {
      "epoch": 0.6668599594085242,
      "grad_norm": 22.359739303588867,
      "learning_rate": 0.00029230919266293035,
      "loss": 35.3832,
      "step": 2300
    },
    {
      "epoch": 0.6726587416642505,
      "grad_norm": 20.652332305908203,
      "learning_rate": 0.00029216315890835166,
      "loss": 34.9732,
      "step": 2320
    },
    {
      "epoch": 0.6784575239199768,
      "grad_norm": 19.30373764038086,
      "learning_rate": 0.0002920157888875284,
      "loss": 34.3895,
      "step": 2340
    },
    {
      "epoch": 0.6842563061757031,
      "grad_norm": 19.764083862304688,
      "learning_rate": 0.0002918670839856688,
      "loss": 34.982,
      "step": 2360
    },
    {
      "epoch": 0.6900550884314294,
      "grad_norm": 21.648548126220703,
      "learning_rate": 0.00029171704560052833,
      "loss": 34.9907,
      "step": 2380
    },
    {
      "epoch": 0.6958538706871557,
      "grad_norm": 20.393932342529297,
      "learning_rate": 0.00029156567514239666,
      "loss": 34.1365,
      "step": 2400
    },
    {
      "epoch": 0.701652652942882,
      "grad_norm": 19.19275665283203,
      "learning_rate": 0.00029141297403408425,
      "loss": 33.9236,
      "step": 2420
    },
    {
      "epoch": 0.7074514351986083,
      "grad_norm": 20.426254272460938,
      "learning_rate": 0.00029125894371090914,
      "loss": 34.2234,
      "step": 2440
    },
    {
      "epoch": 0.7132502174543346,
      "grad_norm": 19.85135269165039,
      "learning_rate": 0.0002911035856206832,
      "loss": 33.8529,
      "step": 2460
    },
    {
      "epoch": 0.7190489997100609,
      "grad_norm": 19.284374237060547,
      "learning_rate": 0.0002909469012236988,
      "loss": 33.7031,
      "step": 2480
    },
    {
      "epoch": 0.7248477819657871,
      "grad_norm": 19.08797836303711,
      "learning_rate": 0.000290788891992715,
      "loss": 33.4118,
      "step": 2500
    },
    {
      "epoch": 0.7306465642215135,
      "grad_norm": 19.277812957763672,
      "learning_rate": 0.0002906295594129435,
      "loss": 32.8969,
      "step": 2520
    },
    {
      "epoch": 0.7364453464772398,
      "grad_norm": 18.978492736816406,
      "learning_rate": 0.000290468904982035,
      "loss": 32.7735,
      "step": 2540
    },
    {
      "epoch": 0.7422441287329661,
      "grad_norm": 31.22699737548828,
      "learning_rate": 0.000290306930210065,
      "loss": 32.3101,
      "step": 2560
    },
    {
      "epoch": 0.7480429109886924,
      "grad_norm": 18.545467376708984,
      "learning_rate": 0.0002901436366195194,
      "loss": 32.5376,
      "step": 2580
    },
    {
      "epoch": 0.7538416932444186,
      "grad_norm": 18.500934600830078,
      "learning_rate": 0.00028997902574528047,
      "loss": 32.254,
      "step": 2600
    },
    {
      "epoch": 0.759640475500145,
      "grad_norm": 18.906076431274414,
      "learning_rate": 0.0002898130991346124,
      "loss": 32.2023,
      "step": 2620
    },
    {
      "epoch": 0.7654392577558713,
      "grad_norm": 18.09461784362793,
      "learning_rate": 0.00028965425157400135,
      "loss": 32.4578,
      "step": 2640
    },
    {
      "epoch": 0.7712380400115976,
      "grad_norm": 20.78375816345215,
      "learning_rate": 0.0002894857637744374,
      "loss": 31.3927,
      "step": 2660
    },
    {
      "epoch": 0.7770368222673238,
      "grad_norm": 20.054664611816406,
      "learning_rate": 0.0002893159648748727,
      "loss": 31.6136,
      "step": 2680
    },
    {
      "epoch": 0.7828356045230501,
      "grad_norm": 20.02074432373047,
      "learning_rate": 0.00028914485647133653,
      "loss": 31.3205,
      "step": 2700
    },
    {
      "epoch": 0.7886343867787765,
      "grad_norm": 17.988508224487305,
      "learning_rate": 0.00028897244017216654,
      "loss": 31.7108,
      "step": 2720
    },
    {
      "epoch": 0.7944331690345028,
      "grad_norm": 19.69158172607422,
      "learning_rate": 0.0002887987175979942,
      "loss": 31.3629,
      "step": 2740
    },
    {
      "epoch": 0.8002319512902291,
      "grad_norm": 17.32233238220215,
      "learning_rate": 0.0002886236903817293,
      "loss": 30.5842,
      "step": 2760
    },
    {
      "epoch": 0.8060307335459553,
      "grad_norm": 17.88759422302246,
      "learning_rate": 0.0002884473601685446,
      "loss": 30.9642,
      "step": 2780
    },
    {
      "epoch": 0.8118295158016816,
      "grad_norm": 16.208253860473633,
      "learning_rate": 0.0002882697286158605,
      "loss": 30.2626,
      "step": 2800
    },
    {
      "epoch": 0.817628298057408,
      "grad_norm": 18.546138763427734,
      "learning_rate": 0.0002880907973933292,
      "loss": 30.9059,
      "step": 2820
    },
    {
      "epoch": 0.8234270803131343,
      "grad_norm": 18.236358642578125,
      "learning_rate": 0.0002879105681828194,
      "loss": 30.082,
      "step": 2840
    },
    {
      "epoch": 0.8292258625688606,
      "grad_norm": 17.598196029663086,
      "learning_rate": 0.0002877290426784002,
      "loss": 30.031,
      "step": 2860
    },
    {
      "epoch": 0.8350246448245868,
      "grad_norm": 19.418621063232422,
      "learning_rate": 0.00028754622258632515,
      "loss": 30.3035,
      "step": 2880
    },
    {
      "epoch": 0.8408234270803131,
      "grad_norm": 17.213825225830078,
      "learning_rate": 0.0002873621096250163,
      "loss": 29.9122,
      "step": 2900
    },
    {
      "epoch": 0.8466222093360394,
      "grad_norm": 18.255823135375977,
      "learning_rate": 0.0002871767055250483,
      "loss": 29.7283,
      "step": 2920
    },
    {
      "epoch": 0.8524209915917658,
      "grad_norm": 17.479957580566406,
      "learning_rate": 0.00028699001202913164,
      "loss": 29.5341,
      "step": 2940
    },
    {
      "epoch": 0.858219773847492,
      "grad_norm": 17.358537673950195,
      "learning_rate": 0.0002868020308920966,
      "loss": 29.5956,
      "step": 2960
    },
    {
      "epoch": 0.8640185561032183,
      "grad_norm": 19.702289581298828,
      "learning_rate": 0.0002866127638808767,
      "loss": 29.6338,
      "step": 2980
    },
    {
      "epoch": 0.8698173383589446,
      "grad_norm": 17.25326919555664,
      "learning_rate": 0.000286422212774492,
      "loss": 29.6277,
      "step": 3000
    },
    {
      "epoch": 0.8756161206146709,
      "grad_norm": 17.14394760131836,
      "learning_rate": 0.0002862303793640326,
      "loss": 29.0151,
      "step": 3020
    },
    {
      "epoch": 0.8814149028703973,
      "grad_norm": 18.343904495239258,
      "learning_rate": 0.0002860372654526415,
      "loss": 28.6991,
      "step": 3040
    },
    {
      "epoch": 0.8872136851261235,
      "grad_norm": 17.536909103393555,
      "learning_rate": 0.00028584287285549775,
      "loss": 28.9071,
      "step": 3060
    },
    {
      "epoch": 0.8930124673818498,
      "grad_norm": 17.17378807067871,
      "learning_rate": 0.0002856472033997996,
      "loss": 29.2576,
      "step": 3080
    },
    {
      "epoch": 0.8988112496375761,
      "grad_norm": 15.23829174041748,
      "learning_rate": 0.0002854502589247472,
      "loss": 28.8214,
      "step": 3100
    },
    {
      "epoch": 0.9046100318933024,
      "grad_norm": 16.882673263549805,
      "learning_rate": 0.00028525204128152496,
      "loss": 28.2866,
      "step": 3120
    },
    {
      "epoch": 0.9104088141490287,
      "grad_norm": 18.19736671447754,
      "learning_rate": 0.0002850525523332848,
      "loss": 28.1073,
      "step": 3140
    },
    {
      "epoch": 0.916207596404755,
      "grad_norm": 16.982070922851562,
      "learning_rate": 0.0002848517939551283,
      "loss": 28.3062,
      "step": 3160
    },
    {
      "epoch": 0.9220063786604813,
      "grad_norm": 15.338855743408203,
      "learning_rate": 0.0002846497680340888,
      "loss": 27.9309,
      "step": 3180
    },
    {
      "epoch": 0.9278051609162076,
      "grad_norm": 15.999330520629883,
      "learning_rate": 0.0002844464764691141,
      "loss": 28.1383,
      "step": 3200
    },
    {
      "epoch": 0.9336039431719338,
      "grad_norm": 18.987632751464844,
      "learning_rate": 0.0002842419211710486,
      "loss": 28.4008,
      "step": 3220
    },
    {
      "epoch": 0.9394027254276602,
      "grad_norm": 15.920620918273926,
      "learning_rate": 0.00028403610406261496,
      "loss": 27.7732,
      "step": 3240
    },
    {
      "epoch": 0.9452015076833865,
      "grad_norm": 19.287921905517578,
      "learning_rate": 0.00028382902707839637,
      "loss": 28.0459,
      "step": 3260
    },
    {
      "epoch": 0.9510002899391128,
      "grad_norm": 17.435701370239258,
      "learning_rate": 0.0002836206921648182,
      "loss": 27.6604,
      "step": 3280
    },
    {
      "epoch": 0.9567990721948391,
      "grad_norm": 16.598896026611328,
      "learning_rate": 0.00028341110128012975,
      "loss": 27.7194,
      "step": 3300
    },
    {
      "epoch": 0.9625978544505653,
      "grad_norm": 16.87171745300293,
      "learning_rate": 0.00028320025639438603,
      "loss": 27.8095,
      "step": 3320
    },
    {
      "epoch": 0.9683966367062917,
      "grad_norm": 15.570245742797852,
      "learning_rate": 0.0002829881594894289,
      "loss": 27.7879,
      "step": 3340
    },
    {
      "epoch": 0.974195418962018,
      "grad_norm": 16.225461959838867,
      "learning_rate": 0.00028277481255886857,
      "loss": 27.5749,
      "step": 3360
    },
    {
      "epoch": 0.9799942012177443,
      "grad_norm": 18.14293098449707,
      "learning_rate": 0.000282560217608065,
      "loss": 27.244,
      "step": 3380
    },
    {
      "epoch": 0.9857929834734706,
      "grad_norm": 15.891436576843262,
      "learning_rate": 0.0002823443766541089,
      "loss": 27.2843,
      "step": 3400
    },
    {
      "epoch": 0.9915917657291968,
      "grad_norm": 19.212257385253906,
      "learning_rate": 0.0002821272917258028,
      "loss": 26.9696,
      "step": 3420
    },
    {
      "epoch": 0.9973905479849232,
      "grad_norm": 15.60649299621582,
      "learning_rate": 0.00028190896486364215,
      "loss": 27.3586,
      "step": 3440
    },
    {
      "epoch": 1.0,
      "eval_loss": 24.93880844116211,
      "eval_runtime": 178.9031,
      "eval_samples_per_second": 45.79,
      "eval_steps_per_second": 5.724,
      "step": 3449
    },
    {
      "epoch": 1.0031893302406494,
      "grad_norm": 16.241317749023438,
      "learning_rate": 0.00028168939811979576,
      "loss": 26.4081,
      "step": 3460
    },
    {
      "epoch": 1.0089881124963758,
      "grad_norm": 15.58671760559082,
      "learning_rate": 0.0002814685935580869,
      "loss": 25.7689,
      "step": 3480
    },
    {
      "epoch": 1.0147868947521022,
      "grad_norm": 18.99723243713379,
      "learning_rate": 0.00028124655325397364,
      "loss": 25.8025,
      "step": 3500
    },
    {
      "epoch": 1.0205856770078283,
      "grad_norm": 15.074291229248047,
      "learning_rate": 0.00028102327929452954,
      "loss": 25.8347,
      "step": 3520
    },
    {
      "epoch": 1.0263844592635547,
      "grad_norm": 17.592126846313477,
      "learning_rate": 0.0002807987737784239,
      "loss": 25.8687,
      "step": 3540
    },
    {
      "epoch": 1.0321832415192809,
      "grad_norm": 15.16993236541748,
      "learning_rate": 0.000280573038815902,
      "loss": 25.3501,
      "step": 3560
    },
    {
      "epoch": 1.0379820237750073,
      "grad_norm": 15.35836410522461,
      "learning_rate": 0.00028034607652876537,
      "loss": 25.2457,
      "step": 3580
    },
    {
      "epoch": 1.0437808060307336,
      "grad_norm": 14.878206253051758,
      "learning_rate": 0.0002801178890503519,
      "loss": 25.659,
      "step": 3600
    },
    {
      "epoch": 1.0495795882864598,
      "grad_norm": 15.188692092895508,
      "learning_rate": 0.0002798884785255156,
      "loss": 25.2452,
      "step": 3620
    },
    {
      "epoch": 1.0553783705421862,
      "grad_norm": 13.96314811706543,
      "learning_rate": 0.0002796578471106066,
      "loss": 25.1681,
      "step": 3640
    },
    {
      "epoch": 1.0611771527979124,
      "grad_norm": 15.15994930267334,
      "learning_rate": 0.00027942599697345066,
      "loss": 25.3322,
      "step": 3660
    },
    {
      "epoch": 1.0669759350536387,
      "grad_norm": 14.140096664428711,
      "learning_rate": 0.0002791929302933292,
      "loss": 25.106,
      "step": 3680
    },
    {
      "epoch": 1.0727747173093651,
      "grad_norm": 14.888823509216309,
      "learning_rate": 0.00027895864926095836,
      "loss": 25.0924,
      "step": 3700
    },
    {
      "epoch": 1.0785734995650913,
      "grad_norm": 14.444758415222168,
      "learning_rate": 0.0002787231560784686,
      "loss": 24.7977,
      "step": 3720
    },
    {
      "epoch": 1.0843722818208177,
      "grad_norm": 16.181468963623047,
      "learning_rate": 0.00027848645295938435,
      "loss": 25.0041,
      "step": 3740
    },
    {
      "epoch": 1.0901710640765438,
      "grad_norm": 15.143677711486816,
      "learning_rate": 0.0002782485421286024,
      "loss": 24.9493,
      "step": 3760
    },
    {
      "epoch": 1.0959698463322702,
      "grad_norm": 14.043436050415039,
      "learning_rate": 0.00027800942582237184,
      "loss": 24.6523,
      "step": 3780
    },
    {
      "epoch": 1.1017686285879966,
      "grad_norm": 14.151511192321777,
      "learning_rate": 0.0002777691062882724,
      "loss": 24.6076,
      "step": 3800
    },
    {
      "epoch": 1.1075674108437228,
      "grad_norm": 16.754718780517578,
      "learning_rate": 0.0002775275857851937,
      "loss": 24.5752,
      "step": 3820
    },
    {
      "epoch": 1.1133661930994492,
      "grad_norm": 15.722203254699707,
      "learning_rate": 0.0002772848665833139,
      "loss": 24.3289,
      "step": 3840
    },
    {
      "epoch": 1.1191649753551753,
      "grad_norm": 15.347890853881836,
      "learning_rate": 0.0002770409509640783,
      "loss": 24.576,
      "step": 3860
    },
    {
      "epoch": 1.1249637576109017,
      "grad_norm": 14.505379676818848,
      "learning_rate": 0.0002767958412201781,
      "loss": 24.3096,
      "step": 3880
    },
    {
      "epoch": 1.1307625398666281,
      "grad_norm": 14.695535659790039,
      "learning_rate": 0.0002765495396555285,
      "loss": 24.2889,
      "step": 3900
    },
    {
      "epoch": 1.1365613221223543,
      "grad_norm": 16.890949249267578,
      "learning_rate": 0.00027630204858524734,
      "loss": 24.5176,
      "step": 3920
    },
    {
      "epoch": 1.1423601043780807,
      "grad_norm": 16.227684020996094,
      "learning_rate": 0.0002760533703356333,
      "loss": 24.2358,
      "step": 3940
    },
    {
      "epoch": 1.1481588866338068,
      "grad_norm": 14.215869903564453,
      "learning_rate": 0.00027580350724414396,
      "loss": 24.3907,
      "step": 3960
    },
    {
      "epoch": 1.1539576688895332,
      "grad_norm": 14.744213104248047,
      "learning_rate": 0.0002755524616593738,
      "loss": 24.0659,
      "step": 3980
    },
    {
      "epoch": 1.1597564511452596,
      "grad_norm": 13.832232475280762,
      "learning_rate": 0.00027530023594103225,
      "loss": 23.9087,
      "step": 4000
    },
    {
      "epoch": 1.1655552334009858,
      "grad_norm": 15.164985656738281,
      "learning_rate": 0.0002750468324599213,
      "loss": 24.1381,
      "step": 4020
    },
    {
      "epoch": 1.1713540156567122,
      "grad_norm": 14.020565032958984,
      "learning_rate": 0.00027479225359791355,
      "loss": 24.0572,
      "step": 4040
    },
    {
      "epoch": 1.1771527979124383,
      "grad_norm": 14.475423812866211,
      "learning_rate": 0.0002745365017479295,
      "loss": 23.8097,
      "step": 4060
    },
    {
      "epoch": 1.1829515801681647,
      "grad_norm": 14.266133308410645,
      "learning_rate": 0.00027427957931391514,
      "loss": 24.0126,
      "step": 4080
    },
    {
      "epoch": 1.188750362423891,
      "grad_norm": 14.114221572875977,
      "learning_rate": 0.00027402148871081953,
      "loss": 23.7736,
      "step": 4100
    },
    {
      "epoch": 1.1945491446796173,
      "grad_norm": 15.280998229980469,
      "learning_rate": 0.0002737622323645718,
      "loss": 23.5428,
      "step": 4120
    },
    {
      "epoch": 1.2003479269353436,
      "grad_norm": 13.78442096710205,
      "learning_rate": 0.00027350181271205867,
      "loss": 23.5226,
      "step": 4140
    },
    {
      "epoch": 1.2061467091910698,
      "grad_norm": 14.823829650878906,
      "learning_rate": 0.0002732402322011013,
      "loss": 23.314,
      "step": 4160
    },
    {
      "epoch": 1.2119454914467962,
      "grad_norm": 13.961451530456543,
      "learning_rate": 0.00027297749329043236,
      "loss": 23.6264,
      "step": 4180
    },
    {
      "epoch": 1.2177442737025226,
      "grad_norm": 14.023921012878418,
      "learning_rate": 0.000272713598449673,
      "loss": 23.3605,
      "step": 4200
    },
    {
      "epoch": 1.2235430559582487,
      "grad_norm": 13.778355598449707,
      "learning_rate": 0.0002724485501593095,
      "loss": 23.5247,
      "step": 4220
    },
    {
      "epoch": 1.2293418382139751,
      "grad_norm": 16.06669807434082,
      "learning_rate": 0.00027218235091067003,
      "loss": 23.2023,
      "step": 4240
    },
    {
      "epoch": 1.2351406204697013,
      "grad_norm": 14.272750854492188,
      "learning_rate": 0.0002719150032059012,
      "loss": 23.5035,
      "step": 4260
    },
    {
      "epoch": 1.2409394027254277,
      "grad_norm": 13.624571800231934,
      "learning_rate": 0.0002716465095579446,
      "loss": 23.2663,
      "step": 4280
    },
    {
      "epoch": 1.246738184981154,
      "grad_norm": 13.159943580627441,
      "learning_rate": 0.00027137687249051325,
      "loss": 23.1936,
      "step": 4300
    },
    {
      "epoch": 1.2525369672368802,
      "grad_norm": 13.857083320617676,
      "learning_rate": 0.00027110609453806755,
      "loss": 23.1515,
      "step": 4320
    },
    {
      "epoch": 1.2583357494926066,
      "grad_norm": 14.966668128967285,
      "learning_rate": 0.0002708341782457918,
      "loss": 23.2535,
      "step": 4340
    },
    {
      "epoch": 1.2641345317483328,
      "grad_norm": 13.579981803894043,
      "learning_rate": 0.00027056112616957007,
      "loss": 23.1602,
      "step": 4360
    },
    {
      "epoch": 1.2699333140040592,
      "grad_norm": 13.955556869506836,
      "learning_rate": 0.00027028694087596243,
      "loss": 22.9426,
      "step": 4380
    },
    {
      "epoch": 1.2757320962597856,
      "grad_norm": 15.038387298583984,
      "learning_rate": 0.00027001162494218054,
      "loss": 22.865,
      "step": 4400
    },
    {
      "epoch": 1.2815308785155117,
      "grad_norm": 14.940750122070312,
      "learning_rate": 0.00026973518095606345,
      "loss": 22.8322,
      "step": 4420
    },
    {
      "epoch": 1.2873296607712381,
      "grad_norm": 13.280243873596191,
      "learning_rate": 0.00026945761151605363,
      "loss": 23.2646,
      "step": 4440
    },
    {
      "epoch": 1.2931284430269643,
      "grad_norm": 13.355721473693848,
      "learning_rate": 0.00026917891923117205,
      "loss": 22.9689,
      "step": 4460
    },
    {
      "epoch": 1.2989272252826907,
      "grad_norm": 15.440045356750488,
      "learning_rate": 0.00026889910672099393,
      "loss": 22.7368,
      "step": 4480
    },
    {
      "epoch": 1.304726007538417,
      "grad_norm": 13.935479164123535,
      "learning_rate": 0.0002686181766156241,
      "loss": 22.8427,
      "step": 4500
    },
    {
      "epoch": 1.3105247897941432,
      "grad_norm": 13.946319580078125,
      "learning_rate": 0.0002683361315556723,
      "loss": 22.6669,
      "step": 4520
    },
    {
      "epoch": 1.3163235720498696,
      "grad_norm": 12.885693550109863,
      "learning_rate": 0.0002680529741922283,
      "loss": 22.6092,
      "step": 4540
    },
    {
      "epoch": 1.3221223543055958,
      "grad_norm": 13.32244873046875,
      "learning_rate": 0.000267768707186837,
      "loss": 22.7067,
      "step": 4560
    },
    {
      "epoch": 1.3279211365613222,
      "grad_norm": 13.961481094360352,
      "learning_rate": 0.0002674833332114733,
      "loss": 22.6121,
      "step": 4580
    },
    {
      "epoch": 1.3337199188170485,
      "grad_norm": 12.937259674072266,
      "learning_rate": 0.00026719685494851724,
      "loss": 22.7427,
      "step": 4600
    },
    {
      "epoch": 1.3395187010727747,
      "grad_norm": 13.618854522705078,
      "learning_rate": 0.0002669092750907284,
      "loss": 22.1659,
      "step": 4620
    },
    {
      "epoch": 1.345317483328501,
      "grad_norm": 13.731297492980957,
      "learning_rate": 0.00026662059634122127,
      "loss": 22.3427,
      "step": 4640
    },
    {
      "epoch": 1.3511162655842273,
      "grad_norm": 13.345263481140137,
      "learning_rate": 0.00026633082141343896,
      "loss": 22.3818,
      "step": 4660
    },
    {
      "epoch": 1.3569150478399536,
      "grad_norm": 12.831385612487793,
      "learning_rate": 0.00026603995303112833,
      "loss": 22.5982,
      "step": 4680
    },
    {
      "epoch": 1.36271383009568,
      "grad_norm": 12.789600372314453,
      "learning_rate": 0.0002657479939283142,
      "loss": 22.3793,
      "step": 4700
    },
    {
      "epoch": 1.3685126123514062,
      "grad_norm": 12.026738166809082,
      "learning_rate": 0.0002654549468492736,
      "loss": 22.2581,
      "step": 4720
    },
    {
      "epoch": 1.3743113946071324,
      "grad_norm": 13.64185905456543,
      "learning_rate": 0.00026516081454851004,
      "loss": 22.3194,
      "step": 4740
    },
    {
      "epoch": 1.3801101768628588,
      "grad_norm": 13.595887184143066,
      "learning_rate": 0.00026486559979072764,
      "loss": 22.3663,
      "step": 4760
    },
    {
      "epoch": 1.3859089591185851,
      "grad_norm": 13.16588020324707,
      "learning_rate": 0.000264569305350805,
      "loss": 22.2626,
      "step": 4780
    },
    {
      "epoch": 1.3917077413743115,
      "grad_norm": 13.668676376342773,
      "learning_rate": 0.00026427193401376927,
      "loss": 22.2558,
      "step": 4800
    },
    {
      "epoch": 1.3975065236300377,
      "grad_norm": 13.086275100708008,
      "learning_rate": 0.0002639734885747699,
      "loss": 21.9455,
      "step": 4820
    },
    {
      "epoch": 1.4033053058857639,
      "grad_norm": 12.911947250366211,
      "learning_rate": 0.0002636739718390526,
      "loss": 21.7115,
      "step": 4840
    },
    {
      "epoch": 1.4091040881414902,
      "grad_norm": 13.571891784667969,
      "learning_rate": 0.00026337338662193245,
      "loss": 21.8428,
      "step": 4860
    },
    {
      "epoch": 1.4149028703972166,
      "grad_norm": 13.200066566467285,
      "learning_rate": 0.0002630717357487678,
      "loss": 21.9381,
      "step": 4880
    },
    {
      "epoch": 1.420701652652943,
      "grad_norm": 12.504344940185547,
      "learning_rate": 0.0002627690220549339,
      "loss": 21.8766,
      "step": 4900
    },
    {
      "epoch": 1.4265004349086692,
      "grad_norm": 13.597160339355469,
      "learning_rate": 0.00026246524838579574,
      "loss": 21.7724,
      "step": 4920
    },
    {
      "epoch": 1.4322992171643953,
      "grad_norm": 13.463672637939453,
      "learning_rate": 0.0002621604175966818,
      "loss": 21.9091,
      "step": 4940
    },
    {
      "epoch": 1.4380979994201217,
      "grad_norm": 13.230565071105957,
      "learning_rate": 0.0002618545325528567,
      "loss": 21.9314,
      "step": 4960
    },
    {
      "epoch": 1.4438967816758481,
      "grad_norm": 12.882989883422852,
      "learning_rate": 0.00026154759612949483,
      "loss": 21.6914,
      "step": 4980
    },
    {
      "epoch": 1.4496955639315745,
      "grad_norm": 14.150004386901855,
      "learning_rate": 0.00026123961121165293,
      "loss": 21.4707,
      "step": 5000
    },
    {
      "epoch": 1.4554943461873007,
      "grad_norm": 13.13420295715332,
      "learning_rate": 0.00026093058069424316,
      "loss": 21.5893,
      "step": 5020
    },
    {
      "epoch": 1.4612931284430268,
      "grad_norm": 12.697307586669922,
      "learning_rate": 0.00026062050748200567,
      "loss": 21.4835,
      "step": 5040
    },
    {
      "epoch": 1.4670919106987532,
      "grad_norm": 12.299413681030273,
      "learning_rate": 0.00026030939448948157,
      "loss": 21.5154,
      "step": 5060
    },
    {
      "epoch": 1.4728906929544796,
      "grad_norm": 13.266225814819336,
      "learning_rate": 0.00025999724464098546,
      "loss": 21.5994,
      "step": 5080
    },
    {
      "epoch": 1.478689475210206,
      "grad_norm": 12.415852546691895,
      "learning_rate": 0.0002596840608705777,
      "loss": 21.4699,
      "step": 5100
    },
    {
      "epoch": 1.4844882574659322,
      "grad_norm": 11.940943717956543,
      "learning_rate": 0.0002593698461220372,
      "loss": 21.4009,
      "step": 5120
    },
    {
      "epoch": 1.4902870397216583,
      "grad_norm": 13.139724731445312,
      "learning_rate": 0.00025905460334883356,
      "loss": 21.5509,
      "step": 5140
    },
    {
      "epoch": 1.4960858219773847,
      "grad_norm": 13.52147102355957,
      "learning_rate": 0.00025873833551409914,
      "loss": 21.1356,
      "step": 5160
    },
    {
      "epoch": 1.501884604233111,
      "grad_norm": 12.826211929321289,
      "learning_rate": 0.0002584210455906017,
      "loss": 21.1407,
      "step": 5180
    },
    {
      "epoch": 1.5076833864888375,
      "grad_norm": 11.67873764038086,
      "learning_rate": 0.0002581027365607159,
      "loss": 21.2711,
      "step": 5200
    },
    {
      "epoch": 1.5134821687445636,
      "grad_norm": 17.970535278320312,
      "learning_rate": 0.0002577834114163956,
      "loss": 21.2227,
      "step": 5220
    },
    {
      "epoch": 1.5192809510002898,
      "grad_norm": 12.16345500946045,
      "learning_rate": 0.00025746307315914566,
      "loss": 21.297,
      "step": 5240
    },
    {
      "epoch": 1.5250797332560162,
      "grad_norm": 12.975347518920898,
      "learning_rate": 0.00025714172479999374,
      "loss": 21.2912,
      "step": 5260
    },
    {
      "epoch": 1.5308785155117426,
      "grad_norm": 11.257718086242676,
      "learning_rate": 0.0002568193693594619,
      "loss": 21.3515,
      "step": 5280
    },
    {
      "epoch": 1.536677297767469,
      "grad_norm": 12.741780281066895,
      "learning_rate": 0.0002564960098675384,
      "loss": 21.2091,
      "step": 5300
    },
    {
      "epoch": 1.5424760800231951,
      "grad_norm": 21.020801544189453,
      "learning_rate": 0.000256171649363649,
      "loss": 21.3383,
      "step": 5320
    },
    {
      "epoch": 1.5482748622789213,
      "grad_norm": 13.150117874145508,
      "learning_rate": 0.0002558462908966285,
      "loss": 20.9288,
      "step": 5340
    },
    {
      "epoch": 1.5540736445346477,
      "grad_norm": 12.463016510009766,
      "learning_rate": 0.00025551993752469227,
      "loss": 20.9276,
      "step": 5360
    },
    {
      "epoch": 1.559872426790374,
      "grad_norm": 12.405658721923828,
      "learning_rate": 0.0002551925923154071,
      "loss": 21.1784,
      "step": 5380
    },
    {
      "epoch": 1.5656712090461005,
      "grad_norm": 12.871915817260742,
      "learning_rate": 0.0002548642583456626,
      "loss": 21.2732,
      "step": 5400
    },
    {
      "epoch": 1.5714699913018266,
      "grad_norm": 12.780670166015625,
      "learning_rate": 0.0002545349387016423,
      "loss": 20.7422,
      "step": 5420
    },
    {
      "epoch": 1.5772687735575528,
      "grad_norm": 12.705175399780273,
      "learning_rate": 0.0002542046364787947,
      "loss": 20.8625,
      "step": 5440
    },
    {
      "epoch": 1.5830675558132792,
      "grad_norm": 11.705506324768066,
      "learning_rate": 0.0002538733547818039,
      "loss": 20.8867,
      "step": 5460
    },
    {
      "epoch": 1.5888663380690056,
      "grad_norm": 11.83054256439209,
      "learning_rate": 0.0002535410967245608,
      "loss": 20.8175,
      "step": 5480
    },
    {
      "epoch": 1.594665120324732,
      "grad_norm": 14.562557220458984,
      "learning_rate": 0.00025320786543013335,
      "loss": 20.8694,
      "step": 5500
    },
    {
      "epoch": 1.6004639025804581,
      "grad_norm": 11.625138282775879,
      "learning_rate": 0.00025287366403073765,
      "loss": 20.6584,
      "step": 5520
    },
    {
      "epoch": 1.6062626848361843,
      "grad_norm": 13.563385009765625,
      "learning_rate": 0.0002525384956677083,
      "loss": 21.0995,
      "step": 5540
    },
    {
      "epoch": 1.6120614670919107,
      "grad_norm": 12.41026782989502,
      "learning_rate": 0.0002522023634914689,
      "loss": 20.8216,
      "step": 5560
    },
    {
      "epoch": 1.617860249347637,
      "grad_norm": 12.472243309020996,
      "learning_rate": 0.00025186527066150253,
      "loss": 20.7367,
      "step": 5580
    },
    {
      "epoch": 1.6236590316033634,
      "grad_norm": 12.67564868927002,
      "learning_rate": 0.0002515272203463217,
      "loss": 20.6378,
      "step": 5600
    },
    {
      "epoch": 1.6294578138590896,
      "grad_norm": 12.35672664642334,
      "learning_rate": 0.00025118821572343907,
      "loss": 20.5964,
      "step": 5620
    },
    {
      "epoch": 1.6352565961148158,
      "grad_norm": 12.113374710083008,
      "learning_rate": 0.00025084825997933723,
      "loss": 20.8256,
      "step": 5640
    },
    {
      "epoch": 1.6410553783705422,
      "grad_norm": 12.848283767700195,
      "learning_rate": 0.0002505073563094389,
      "loss": 20.7205,
      "step": 5660
    },
    {
      "epoch": 1.6468541606262685,
      "grad_norm": 12.80556583404541,
      "learning_rate": 0.0002501655079180768,
      "loss": 20.7268,
      "step": 5680
    },
    {
      "epoch": 1.652652942881995,
      "grad_norm": 11.095462799072266,
      "learning_rate": 0.00024982271801846366,
      "loss": 20.6068,
      "step": 5700
    },
    {
      "epoch": 1.658451725137721,
      "grad_norm": 12.059122085571289,
      "learning_rate": 0.0002494789898326619,
      "loss": 20.3524,
      "step": 5720
    },
    {
      "epoch": 1.6642505073934473,
      "grad_norm": 13.471105575561523,
      "learning_rate": 0.00024913432659155337,
      "loss": 20.5128,
      "step": 5740
    },
    {
      "epoch": 1.6700492896491737,
      "grad_norm": 13.133063316345215,
      "learning_rate": 0.000248788731534809,
      "loss": 20.5441,
      "step": 5760
    },
    {
      "epoch": 1.6758480719049,
      "grad_norm": 12.23576545715332,
      "learning_rate": 0.0002484422079108583,
      "loss": 20.3491,
      "step": 5780
    },
    {
      "epoch": 1.6816468541606264,
      "grad_norm": 11.730731964111328,
      "learning_rate": 0.00024809475897685914,
      "loss": 20.6029,
      "step": 5800
    },
    {
      "epoch": 1.6874456364163526,
      "grad_norm": 11.339008331298828,
      "learning_rate": 0.0002477463879986664,
      "loss": 20.3337,
      "step": 5820
    },
    {
      "epoch": 1.6932444186720788,
      "grad_norm": 12.703024864196777,
      "learning_rate": 0.0002473970982508022,
      "loss": 20.1625,
      "step": 5840
    },
    {
      "epoch": 1.6990432009278051,
      "grad_norm": 12.394980430603027,
      "learning_rate": 0.00024704689301642427,
      "loss": 20.2796,
      "step": 5860
    },
    {
      "epoch": 1.7048419831835315,
      "grad_norm": 11.994498252868652,
      "learning_rate": 0.00024669577558729573,
      "loss": 20.3732,
      "step": 5880
    },
    {
      "epoch": 1.710640765439258,
      "grad_norm": 12.318477630615234,
      "learning_rate": 0.00024634374926375385,
      "loss": 20.5883,
      "step": 5900
    },
    {
      "epoch": 1.716439547694984,
      "grad_norm": 11.589080810546875,
      "learning_rate": 0.0002459908173546791,
      "loss": 20.4794,
      "step": 5920
    },
    {
      "epoch": 1.7222383299507102,
      "grad_norm": 14.142841339111328,
      "learning_rate": 0.00024563698317746384,
      "loss": 20.3761,
      "step": 5940
    },
    {
      "epoch": 1.7280371122064366,
      "grad_norm": 11.85220718383789,
      "learning_rate": 0.0002452822500579815,
      "loss": 20.0672,
      "step": 5960
    },
    {
      "epoch": 1.733835894462163,
      "grad_norm": 11.25861644744873,
      "learning_rate": 0.0002449266213305552,
      "loss": 20.1024,
      "step": 5980
    },
    {
      "epoch": 1.7396346767178894,
      "grad_norm": 12.143383979797363,
      "learning_rate": 0.0002445701003379262,
      "loss": 19.9848,
      "step": 6000
    },
    {
      "epoch": 1.7454334589736156,
      "grad_norm": 12.00528621673584,
      "learning_rate": 0.00024421269043122266,
      "loss": 20.1422,
      "step": 6020
    },
    {
      "epoch": 1.7512322412293417,
      "grad_norm": 11.770750045776367,
      "learning_rate": 0.00024385439496992813,
      "loss": 19.9323,
      "step": 6040
    },
    {
      "epoch": 1.7570310234850681,
      "grad_norm": 13.940559387207031,
      "learning_rate": 0.00024349521732184995,
      "loss": 20.218,
      "step": 6060
    },
    {
      "epoch": 1.7628298057407945,
      "grad_norm": 13.3436918258667,
      "learning_rate": 0.0002431351608630876,
      "loss": 20.2602,
      "step": 6080
    },
    {
      "epoch": 1.768628587996521,
      "grad_norm": 11.656862258911133,
      "learning_rate": 0.00024277422897800102,
      "loss": 20.1109,
      "step": 6100
    },
    {
      "epoch": 1.774427370252247,
      "grad_norm": 14.478408813476562,
      "learning_rate": 0.0002424124250591786,
      "loss": 20.1769,
      "step": 6120
    },
    {
      "epoch": 1.7802261525079732,
      "grad_norm": 12.050440788269043,
      "learning_rate": 0.00024204975250740555,
      "loss": 20.1077,
      "step": 6140
    },
    {
      "epoch": 1.7860249347636996,
      "grad_norm": 11.160136222839355,
      "learning_rate": 0.0002416862147316318,
      "loss": 19.821,
      "step": 6160
    },
    {
      "epoch": 1.791823717019426,
      "grad_norm": 11.417317390441895,
      "learning_rate": 0.00024132181514894,
      "loss": 19.966,
      "step": 6180
    },
    {
      "epoch": 1.7976224992751522,
      "grad_norm": 10.662832260131836,
      "learning_rate": 0.00024095655718451325,
      "loss": 19.8167,
      "step": 6200
    },
    {
      "epoch": 1.8034212815308786,
      "grad_norm": 11.76180648803711,
      "learning_rate": 0.00024059044427160323,
      "loss": 20.0162,
      "step": 6220
    },
    {
      "epoch": 1.8092200637866047,
      "grad_norm": 11.400026321411133,
      "learning_rate": 0.00024022347985149753,
      "loss": 19.9051,
      "step": 6240
    },
    {
      "epoch": 1.815018846042331,
      "grad_norm": 11.06086254119873,
      "learning_rate": 0.00023985566737348758,
      "loss": 19.5427,
      "step": 6260
    },
    {
      "epoch": 1.8208176282980575,
      "grad_norm": 10.727008819580078,
      "learning_rate": 0.00023948701029483622,
      "loss": 19.7242,
      "step": 6280
    },
    {
      "epoch": 1.8266164105537837,
      "grad_norm": 11.008373260498047,
      "learning_rate": 0.00023911751208074508,
      "loss": 19.5284,
      "step": 6300
    },
    {
      "epoch": 1.83241519280951,
      "grad_norm": 12.895249366760254,
      "learning_rate": 0.00023874717620432208,
      "loss": 19.8091,
      "step": 6320
    },
    {
      "epoch": 1.8382139750652362,
      "grad_norm": 12.385747909545898,
      "learning_rate": 0.00023837600614654868,
      "loss": 19.5206,
      "step": 6340
    },
    {
      "epoch": 1.8440127573209626,
      "grad_norm": 12.55882453918457,
      "learning_rate": 0.00023800400539624743,
      "loss": 19.5927,
      "step": 6360
    },
    {
      "epoch": 1.849811539576689,
      "grad_norm": 11.217458724975586,
      "learning_rate": 0.00023763117745004872,
      "loss": 19.7761,
      "step": 6380
    },
    {
      "epoch": 1.8556103218324151,
      "grad_norm": 13.883374214172363,
      "learning_rate": 0.00023725752581235852,
      "loss": 19.6174,
      "step": 6400
    },
    {
      "epoch": 1.8614091040881415,
      "grad_norm": 11.84191608428955,
      "learning_rate": 0.0002368830539953249,
      "loss": 19.4833,
      "step": 6420
    },
    {
      "epoch": 1.8672078863438677,
      "grad_norm": 11.731230735778809,
      "learning_rate": 0.00023650776551880522,
      "loss": 19.6202,
      "step": 6440
    },
    {
      "epoch": 1.873006668599594,
      "grad_norm": 12.188737869262695,
      "learning_rate": 0.0002361316639103332,
      "loss": 19.7061,
      "step": 6460
    },
    {
      "epoch": 1.8788054508553205,
      "grad_norm": 11.603609085083008,
      "learning_rate": 0.00023575475270508543,
      "loss": 19.5061,
      "step": 6480
    },
    {
      "epoch": 1.8846042331110466,
      "grad_norm": 11.566814422607422,
      "learning_rate": 0.0002353770354458484,
      "loss": 19.4253,
      "step": 6500
    },
    {
      "epoch": 1.890403015366773,
      "grad_norm": 11.665943145751953,
      "learning_rate": 0.00023499851568298523,
      "loss": 19.4946,
      "step": 6520
    },
    {
      "epoch": 1.8962017976224992,
      "grad_norm": 10.741954803466797,
      "learning_rate": 0.0002346191969744021,
      "loss": 19.4824,
      "step": 6540
    },
    {
      "epoch": 1.9020005798782256,
      "grad_norm": 10.554604530334473,
      "learning_rate": 0.0002342390828855148,
      "loss": 19.3134,
      "step": 6560
    },
    {
      "epoch": 1.907799362133952,
      "grad_norm": 10.869955062866211,
      "learning_rate": 0.00023385817698921553,
      "loss": 19.2716,
      "step": 6580
    },
    {
      "epoch": 1.9135981443896781,
      "grad_norm": 11.352387428283691,
      "learning_rate": 0.00023347648286583893,
      "loss": 19.4325,
      "step": 6600
    },
    {
      "epoch": 1.9193969266454045,
      "grad_norm": 10.733734130859375,
      "learning_rate": 0.0002330940041031286,
      "loss": 19.3153,
      "step": 6620
    },
    {
      "epoch": 1.9251957089011307,
      "grad_norm": 10.938465118408203,
      "learning_rate": 0.00023271074429620368,
      "loss": 19.3617,
      "step": 6640
    },
    {
      "epoch": 1.930994491156857,
      "grad_norm": 13.669295310974121,
      "learning_rate": 0.00023232670704752427,
      "loss": 19.2406,
      "step": 6660
    },
    {
      "epoch": 1.9367932734125834,
      "grad_norm": 10.92690658569336,
      "learning_rate": 0.0002319418959668584,
      "loss": 19.3839,
      "step": 6680
    },
    {
      "epoch": 1.9425920556683096,
      "grad_norm": 10.793343544006348,
      "learning_rate": 0.00023155631467124767,
      "loss": 19.2198,
      "step": 6700
    },
    {
      "epoch": 1.948390837924036,
      "grad_norm": 10.485595703125,
      "learning_rate": 0.0002311699667849733,
      "loss": 19.1493,
      "step": 6720
    },
    {
      "epoch": 1.9541896201797622,
      "grad_norm": 11.511152267456055,
      "learning_rate": 0.00023078285593952207,
      "loss": 19.0335,
      "step": 6740
    },
    {
      "epoch": 1.9599884024354886,
      "grad_norm": 14.380446434020996,
      "learning_rate": 0.0002303949857735524,
      "loss": 19.1009,
      "step": 6760
    },
    {
      "epoch": 1.965787184691215,
      "grad_norm": 11.408476829528809,
      "learning_rate": 0.00023000635993285966,
      "loss": 19.2163,
      "step": 6780
    },
    {
      "epoch": 1.971585966946941,
      "grad_norm": 10.716041564941406,
      "learning_rate": 0.00022961698207034248,
      "loss": 19.1277,
      "step": 6800
    },
    {
      "epoch": 1.9773847492026675,
      "grad_norm": 10.823993682861328,
      "learning_rate": 0.00022924637987419995,
      "loss": 19.1946,
      "step": 6820
    },
    {
      "epoch": 1.9831835314583937,
      "grad_norm": 11.141982078552246,
      "learning_rate": 0.0002288555461025129,
      "loss": 19.0518,
      "step": 6840
    },
    {
      "epoch": 1.98898231371412,
      "grad_norm": 11.802702903747559,
      "learning_rate": 0.00022846397112610538,
      "loss": 19.0261,
      "step": 6860
    },
    {
      "epoch": 1.9947810959698464,
      "grad_norm": 12.085564613342285,
      "learning_rate": 0.00022807165862559638,
      "loss": 19.0523,
      "step": 6880
    },
    {
      "epoch": 2.0,
      "eval_loss": 18.015300750732422,
      "eval_runtime": 178.7884,
      "eval_samples_per_second": 45.82,
      "eval_steps_per_second": 5.727,
      "step": 6898
    },
    {
      "epoch": 2.000579878225573,
      "grad_norm": 10.054140090942383,
      "learning_rate": 0.00022769828197700316,
      "loss": 19.2052,
      "step": 6900
    },
    {
      "epoch": 2.0063786604812988,
      "grad_norm": 9.894026756286621,
      "learning_rate": 0.00022730454191709882,
      "loss": 18.2809,
      "step": 6920
    },
    {
      "epoch": 2.012177442737025,
      "grad_norm": 10.159515380859375,
      "learning_rate": 0.00022691007523117752,
      "loss": 18.1749,
      "step": 6940
    },
    {
      "epoch": 2.0179762249927515,
      "grad_norm": 10.21249771118164,
      "learning_rate": 0.0002265148856270389,
      "loss": 18.2885,
      "step": 6960
    },
    {
      "epoch": 2.023775007248478,
      "grad_norm": 10.071005821228027,
      "learning_rate": 0.0002261189768192778,
      "loss": 18.2391,
      "step": 6980
    },
    {
      "epoch": 2.0295737895042043,
      "grad_norm": 10.7509765625,
      "learning_rate": 0.00022572235252924904,
      "loss": 18.1077,
      "step": 7000
    },
    {
      "epoch": 2.0353725717599302,
      "grad_norm": 9.929262161254883,
      "learning_rate": 0.00022532501648503294,
      "loss": 18.515,
      "step": 7020
    },
    {
      "epoch": 2.0411713540156566,
      "grad_norm": 10.217133522033691,
      "learning_rate": 0.0002249269724213997,
      "loss": 18.0128,
      "step": 7040
    },
    {
      "epoch": 2.046970136271383,
      "grad_norm": 10.981317520141602,
      "learning_rate": 0.00022452822407977472,
      "loss": 18.1249,
      "step": 7060
    },
    {
      "epoch": 2.0527689185271094,
      "grad_norm": 10.13853931427002,
      "learning_rate": 0.00022412877520820316,
      "loss": 18.1306,
      "step": 7080
    },
    {
      "epoch": 2.058567700782836,
      "grad_norm": 10.02226734161377,
      "learning_rate": 0.00022372862956131494,
      "loss": 18.2319,
      "step": 7100
    },
    {
      "epoch": 2.0643664830385617,
      "grad_norm": 9.7372407913208,
      "learning_rate": 0.00022332779090028927,
      "loss": 18.1982,
      "step": 7120
    },
    {
      "epoch": 2.070165265294288,
      "grad_norm": 10.714041709899902,
      "learning_rate": 0.00022292626299281944,
      "loss": 18.1107,
      "step": 7140
    },
    {
      "epoch": 2.0759640475500145,
      "grad_norm": 9.8936767578125,
      "learning_rate": 0.0002225240496130772,
      "loss": 18.0504,
      "step": 7160
    },
    {
      "epoch": 2.081762829805741,
      "grad_norm": 12.199867248535156,
      "learning_rate": 0.00022212115454167753,
      "loss": 18.1677,
      "step": 7180
    },
    {
      "epoch": 2.0875616120614673,
      "grad_norm": 10.078747749328613,
      "learning_rate": 0.0002217175815656429,
      "loss": 18.0774,
      "step": 7200
    },
    {
      "epoch": 2.0933603943171932,
      "grad_norm": 9.588126182556152,
      "learning_rate": 0.00022131333447836788,
      "loss": 18.036,
      "step": 7220
    },
    {
      "epoch": 2.0991591765729196,
      "grad_norm": 9.884185791015625,
      "learning_rate": 0.0002209084170795833,
      "loss": 18.0693,
      "step": 7240
    },
    {
      "epoch": 2.104957958828646,
      "grad_norm": 19.920900344848633,
      "learning_rate": 0.00022050283317532056,
      "loss": 18.1885,
      "step": 7260
    },
    {
      "epoch": 2.1107567410843724,
      "grad_norm": 10.967397689819336,
      "learning_rate": 0.00022009658657787593,
      "loss": 18.0521,
      "step": 7280
    },
    {
      "epoch": 2.1165555233400988,
      "grad_norm": 12.241636276245117,
      "learning_rate": 0.00021968968110577466,
      "loss": 18.1295,
      "step": 7300
    },
    {
      "epoch": 2.1223543055958247,
      "grad_norm": 10.269875526428223,
      "learning_rate": 0.00021928212058373514,
      "loss": 17.9412,
      "step": 7320
    },
    {
      "epoch": 2.128153087851551,
      "grad_norm": 10.414010047912598,
      "learning_rate": 0.000218873908842633,
      "loss": 17.8609,
      "step": 7340
    },
    {
      "epoch": 2.1339518701072775,
      "grad_norm": 8.920416831970215,
      "learning_rate": 0.0002184650497194648,
      "loss": 17.8941,
      "step": 7360
    },
    {
      "epoch": 2.139750652363004,
      "grad_norm": 11.696745872497559,
      "learning_rate": 0.00021805554705731244,
      "loss": 17.9218,
      "step": 7380
    },
    {
      "epoch": 2.1455494346187303,
      "grad_norm": 10.604676246643066,
      "learning_rate": 0.00021764540470530655,
      "loss": 18.1134,
      "step": 7400
    },
    {
      "epoch": 2.151348216874456,
      "grad_norm": 10.652936935424805,
      "learning_rate": 0.0002172346265185907,
      "loss": 18.0566,
      "step": 7420
    },
    {
      "epoch": 2.1571469991301826,
      "grad_norm": 10.524480819702148,
      "learning_rate": 0.00021682321635828496,
      "loss": 17.8312,
      "step": 7440
    },
    {
      "epoch": 2.162945781385909,
      "grad_norm": 9.4868745803833,
      "learning_rate": 0.0002164111780914496,
      "loss": 17.9115,
      "step": 7460
    },
    {
      "epoch": 2.1687445636416354,
      "grad_norm": 10.0235595703125,
      "learning_rate": 0.00021599851559104887,
      "loss": 17.9573,
      "step": 7480
    },
    {
      "epoch": 2.1745433458973618,
      "grad_norm": 9.7152099609375,
      "learning_rate": 0.00021558523273591451,
      "loss": 17.6974,
      "step": 7500
    },
    {
      "epoch": 2.1803421281530877,
      "grad_norm": 9.607255935668945,
      "learning_rate": 0.00021517133341070926,
      "loss": 17.7732,
      "step": 7520
    },
    {
      "epoch": 2.186140910408814,
      "grad_norm": 10.32584285736084,
      "learning_rate": 0.00021475682150589033,
      "loss": 17.8781,
      "step": 7540
    },
    {
      "epoch": 2.1919396926645405,
      "grad_norm": 9.518928527832031,
      "learning_rate": 0.00021434170091767302,
      "loss": 17.7532,
      "step": 7560
    },
    {
      "epoch": 2.197738474920267,
      "grad_norm": 10.327202796936035,
      "learning_rate": 0.00021392597554799396,
      "loss": 17.8196,
      "step": 7580
    },
    {
      "epoch": 2.2035372571759932,
      "grad_norm": 11.297865867614746,
      "learning_rate": 0.0002135096493044743,
      "loss": 17.8055,
      "step": 7600
    },
    {
      "epoch": 2.209336039431719,
      "grad_norm": 11.372652053833008,
      "learning_rate": 0.00021309272610038325,
      "loss": 17.5489,
      "step": 7620
    },
    {
      "epoch": 2.2151348216874456,
      "grad_norm": 10.244454383850098,
      "learning_rate": 0.00021267520985460112,
      "loss": 17.6931,
      "step": 7640
    },
    {
      "epoch": 2.220933603943172,
      "grad_norm": 10.578703880310059,
      "learning_rate": 0.0002122571044915825,
      "loss": 17.9354,
      "step": 7660
    },
    {
      "epoch": 2.2267323861988984,
      "grad_norm": 9.812236785888672,
      "learning_rate": 0.0002118384139413194,
      "loss": 17.6601,
      "step": 7680
    },
    {
      "epoch": 2.2325311684546243,
      "grad_norm": 10.143413543701172,
      "learning_rate": 0.00021141914213930447,
      "loss": 17.6076,
      "step": 7700
    },
    {
      "epoch": 2.2383299507103507,
      "grad_norm": 9.671826362609863,
      "learning_rate": 0.00021099929302649358,
      "loss": 17.5588,
      "step": 7720
    },
    {
      "epoch": 2.244128732966077,
      "grad_norm": 9.709762573242188,
      "learning_rate": 0.00021057887054926934,
      "loss": 17.7389,
      "step": 7740
    },
    {
      "epoch": 2.2499275152218035,
      "grad_norm": 10.03208065032959,
      "learning_rate": 0.00021015787865940344,
      "loss": 17.6367,
      "step": 7760
    },
    {
      "epoch": 2.25572629747753,
      "grad_norm": 9.105481147766113,
      "learning_rate": 0.00020973632131401996,
      "loss": 17.5695,
      "step": 7780
    },
    {
      "epoch": 2.2615250797332562,
      "grad_norm": 10.17837142944336,
      "learning_rate": 0.00020931420247555796,
      "loss": 17.4326,
      "step": 7800
    },
    {
      "epoch": 2.267323861988982,
      "grad_norm": 10.482746124267578,
      "learning_rate": 0.0002088915261117342,
      "loss": 17.643,
      "step": 7820
    },
    {
      "epoch": 2.2731226442447086,
      "grad_norm": 9.9923677444458,
      "learning_rate": 0.000208468296195506,
      "loss": 17.4531,
      "step": 7840
    },
    {
      "epoch": 2.278921426500435,
      "grad_norm": 10.839082717895508,
      "learning_rate": 0.00020804451670503377,
      "loss": 17.4653,
      "step": 7860
    },
    {
      "epoch": 2.2847202087561613,
      "grad_norm": 11.66041374206543,
      "learning_rate": 0.00020762019162364365,
      "loss": 17.2944,
      "step": 7880
    },
    {
      "epoch": 2.2905189910118873,
      "grad_norm": 10.626800537109375,
      "learning_rate": 0.00020719532493979005,
      "loss": 17.575,
      "step": 7900
    },
    {
      "epoch": 2.2963177732676137,
      "grad_norm": 10.215069770812988,
      "learning_rate": 0.00020676992064701824,
      "loss": 17.3934,
      "step": 7920
    },
    {
      "epoch": 2.30211655552334,
      "grad_norm": 9.811633110046387,
      "learning_rate": 0.0002063439827439268,
      "loss": 17.4088,
      "step": 7940
    },
    {
      "epoch": 2.3079153377790664,
      "grad_norm": 9.26231861114502,
      "learning_rate": 0.0002059175152341299,
      "loss": 17.587,
      "step": 7960
    },
    {
      "epoch": 2.313714120034793,
      "grad_norm": 9.560110092163086,
      "learning_rate": 0.00020549052212621971,
      "loss": 17.4606,
      "step": 7980
    },
    {
      "epoch": 2.319512902290519,
      "grad_norm": 10.741941452026367,
      "learning_rate": 0.000205063007433729,
      "loss": 17.4953,
      "step": 8000
    },
    {
      "epoch": 2.325311684546245,
      "grad_norm": 10.37015438079834,
      "learning_rate": 0.00020463497517509292,
      "loss": 17.3086,
      "step": 8020
    },
    {
      "epoch": 2.3311104668019715,
      "grad_norm": 10.067212104797363,
      "learning_rate": 0.0002042064293736117,
      "loss": 17.4198,
      "step": 8040
    },
    {
      "epoch": 2.336909249057698,
      "grad_norm": 10.470338821411133,
      "learning_rate": 0.00020377737405741247,
      "loss": 17.444,
      "step": 8060
    },
    {
      "epoch": 2.3427080313134243,
      "grad_norm": 9.264558792114258,
      "learning_rate": 0.00020334781325941176,
      "loss": 17.3227,
      "step": 8080
    },
    {
      "epoch": 2.3485068135691503,
      "grad_norm": 10.577908515930176,
      "learning_rate": 0.0002029177510172772,
      "loss": 17.3653,
      "step": 8100
    },
    {
      "epoch": 2.3543055958248766,
      "grad_norm": 9.844182968139648,
      "learning_rate": 0.00020248719137338982,
      "loss": 17.2198,
      "step": 8120
    },
    {
      "epoch": 2.360104378080603,
      "grad_norm": 10.053077697753906,
      "learning_rate": 0.000202056138374806,
      "loss": 17.2412,
      "step": 8140
    },
    {
      "epoch": 2.3659031603363294,
      "grad_norm": 9.625344276428223,
      "learning_rate": 0.0002016245960732194,
      "loss": 17.4389,
      "step": 8160
    },
    {
      "epoch": 2.371701942592056,
      "grad_norm": 10.321233749389648,
      "learning_rate": 0.00020119256852492293,
      "loss": 17.4723,
      "step": 8180
    },
    {
      "epoch": 2.377500724847782,
      "grad_norm": 9.47790813446045,
      "learning_rate": 0.00020076005979077056,
      "loss": 17.3284,
      "step": 8200
    },
    {
      "epoch": 2.383299507103508,
      "grad_norm": 10.220078468322754,
      "learning_rate": 0.0002003270739361392,
      "loss": 17.1287,
      "step": 8220
    },
    {
      "epoch": 2.3890982893592345,
      "grad_norm": 9.767101287841797,
      "learning_rate": 0.00019989361503089045,
      "loss": 17.283,
      "step": 8240
    },
    {
      "epoch": 2.394897071614961,
      "grad_norm": 9.715044975280762,
      "learning_rate": 0.00019945968714933236,
      "loss": 17.1544,
      "step": 8260
    },
    {
      "epoch": 2.4006958538706873,
      "grad_norm": 10.686688423156738,
      "learning_rate": 0.0001990252943701811,
      "loss": 17.3255,
      "step": 8280
    },
    {
      "epoch": 2.4064946361264132,
      "grad_norm": 9.95347785949707,
      "learning_rate": 0.00019859044077652276,
      "loss": 17.3309,
      "step": 8300
    },
    {
      "epoch": 2.4122934183821396,
      "grad_norm": 10.796809196472168,
      "learning_rate": 0.00019815513045577483,
      "loss": 17.098,
      "step": 8320
    },
    {
      "epoch": 2.418092200637866,
      "grad_norm": 9.18942928314209,
      "learning_rate": 0.0001977193674996477,
      "loss": 17.1357,
      "step": 8340
    },
    {
      "epoch": 2.4238909828935924,
      "grad_norm": 11.727130889892578,
      "learning_rate": 0.00019728315600410644,
      "loss": 17.2472,
      "step": 8360
    },
    {
      "epoch": 2.429689765149319,
      "grad_norm": 10.23834228515625,
      "learning_rate": 0.00019684650006933217,
      "loss": 16.9974,
      "step": 8380
    },
    {
      "epoch": 2.435488547405045,
      "grad_norm": 9.703091621398926,
      "learning_rate": 0.0001964094037996835,
      "loss": 17.2663,
      "step": 8400
    },
    {
      "epoch": 2.441287329660771,
      "grad_norm": 10.616159439086914,
      "learning_rate": 0.00019597187130365802,
      "loss": 16.9521,
      "step": 8420
    },
    {
      "epoch": 2.4470861119164975,
      "grad_norm": 9.673478126525879,
      "learning_rate": 0.00019553390669385356,
      "loss": 17.1523,
      "step": 8440
    },
    {
      "epoch": 2.452884894172224,
      "grad_norm": 9.132913589477539,
      "learning_rate": 0.00019509551408692968,
      "loss": 17.0992,
      "step": 8460
    },
    {
      "epoch": 2.4586836764279503,
      "grad_norm": 9.761923789978027,
      "learning_rate": 0.00019465669760356885,
      "loss": 17.1847,
      "step": 8480
    },
    {
      "epoch": 2.464482458683676,
      "grad_norm": 9.434941291809082,
      "learning_rate": 0.00019421746136843787,
      "loss": 17.1083,
      "step": 8500
    },
    {
      "epoch": 2.4702812409394026,
      "grad_norm": 9.934744834899902,
      "learning_rate": 0.00019377780951014892,
      "loss": 17.1341,
      "step": 8520
    },
    {
      "epoch": 2.476080023195129,
      "grad_norm": 11.141093254089355,
      "learning_rate": 0.00019333774616122094,
      "loss": 17.0428,
      "step": 8540
    },
    {
      "epoch": 2.4818788054508554,
      "grad_norm": 9.77617073059082,
      "learning_rate": 0.00019289727545804053,
      "loss": 17.1437,
      "step": 8560
    },
    {
      "epoch": 2.4876775877065818,
      "grad_norm": 9.60837173461914,
      "learning_rate": 0.00019245640154082333,
      "loss": 17.0279,
      "step": 8580
    },
    {
      "epoch": 2.493476369962308,
      "grad_norm": 10.479777336120605,
      "learning_rate": 0.00019201512855357506,
      "loss": 16.9423,
      "step": 8600
    },
    {
      "epoch": 2.499275152218034,
      "grad_norm": 9.69035816192627,
      "learning_rate": 0.0001915734606440524,
      "loss": 16.8491,
      "step": 8620
    },
    {
      "epoch": 2.5050739344737605,
      "grad_norm": 9.192227363586426,
      "learning_rate": 0.00019113140196372417,
      "loss": 16.9718,
      "step": 8640
    },
    {
      "epoch": 2.510872716729487,
      "grad_norm": 9.60075855255127,
      "learning_rate": 0.00019068895666773218,
      "loss": 16.917,
      "step": 8660
    },
    {
      "epoch": 2.5166714989852133,
      "grad_norm": 10.384331703186035,
      "learning_rate": 0.00019024612891485238,
      "loss": 16.8386,
      "step": 8680
    },
    {
      "epoch": 2.522470281240939,
      "grad_norm": 9.680517196655273,
      "learning_rate": 0.00018980292286745545,
      "loss": 16.8283,
      "step": 8700
    },
    {
      "epoch": 2.5282690634966656,
      "grad_norm": 10.601173400878906,
      "learning_rate": 0.000189359342691468,
      "loss": 17.0217,
      "step": 8720
    },
    {
      "epoch": 2.534067845752392,
      "grad_norm": 9.101758003234863,
      "learning_rate": 0.00018891539255633323,
      "loss": 16.725,
      "step": 8740
    },
    {
      "epoch": 2.5398666280081184,
      "grad_norm": 9.64492416381836,
      "learning_rate": 0.00018847107663497178,
      "loss": 16.8962,
      "step": 8760
    },
    {
      "epoch": 2.5456654102638447,
      "grad_norm": 9.671781539916992,
      "learning_rate": 0.00018802639910374253,
      "loss": 16.9978,
      "step": 8780
    },
    {
      "epoch": 2.551464192519571,
      "grad_norm": 10.05927848815918,
      "learning_rate": 0.00018758136414240327,
      "loss": 16.8874,
      "step": 8800
    },
    {
      "epoch": 2.557262974775297,
      "grad_norm": 9.615530014038086,
      "learning_rate": 0.0001871359759340715,
      "loss": 16.8671,
      "step": 8820
    },
    {
      "epoch": 2.5630617570310235,
      "grad_norm": 9.65673542022705,
      "learning_rate": 0.00018669023866518502,
      "loss": 16.7212,
      "step": 8840
    },
    {
      "epoch": 2.56886053928675,
      "grad_norm": 9.866559982299805,
      "learning_rate": 0.00018624415652546274,
      "loss": 16.6618,
      "step": 8860
    },
    {
      "epoch": 2.5746593215424762,
      "grad_norm": 9.26153564453125,
      "learning_rate": 0.0001857977337078651,
      "loss": 16.8941,
      "step": 8880
    },
    {
      "epoch": 2.580458103798202,
      "grad_norm": 8.507493019104004,
      "learning_rate": 0.0001853509744085548,
      "loss": 16.5859,
      "step": 8900
    },
    {
      "epoch": 2.5862568860539286,
      "grad_norm": 10.699895858764648,
      "learning_rate": 0.0001849038828268573,
      "loss": 16.7124,
      "step": 8920
    },
    {
      "epoch": 2.592055668309655,
      "grad_norm": 11.194765090942383,
      "learning_rate": 0.00018445646316522126,
      "loss": 16.8083,
      "step": 8940
    },
    {
      "epoch": 2.5978544505653813,
      "grad_norm": 9.175727844238281,
      "learning_rate": 0.00018400871962917934,
      "loss": 16.6862,
      "step": 8960
    },
    {
      "epoch": 2.6036532328211077,
      "grad_norm": 9.995468139648438,
      "learning_rate": 0.00018356065642730822,
      "loss": 16.7813,
      "step": 8980
    },
    {
      "epoch": 2.609452015076834,
      "grad_norm": 9.414971351623535,
      "learning_rate": 0.00018311227777118957,
      "loss": 16.5732,
      "step": 9000
    },
    {
      "epoch": 2.61525079733256,
      "grad_norm": 10.615195274353027,
      "learning_rate": 0.0001826635878753699,
      "loss": 16.7678,
      "step": 9020
    },
    {
      "epoch": 2.6210495795882864,
      "grad_norm": 9.157211303710938,
      "learning_rate": 0.0001822145909573214,
      "loss": 16.504,
      "step": 9040
    },
    {
      "epoch": 2.626848361844013,
      "grad_norm": 9.22347354888916,
      "learning_rate": 0.00018176529123740208,
      "loss": 16.577,
      "step": 9060
    },
    {
      "epoch": 2.632647144099739,
      "grad_norm": 9.007606506347656,
      "learning_rate": 0.00018131569293881612,
      "loss": 16.6013,
      "step": 9080
    },
    {
      "epoch": 2.638445926355465,
      "grad_norm": 8.911885261535645,
      "learning_rate": 0.00018086580028757416,
      "loss": 16.638,
      "step": 9100
    },
    {
      "epoch": 2.6442447086111915,
      "grad_norm": 9.68002700805664,
      "learning_rate": 0.0001804156175124538,
      "loss": 16.6582,
      "step": 9120
    },
    {
      "epoch": 2.650043490866918,
      "grad_norm": 11.316798210144043,
      "learning_rate": 0.00017996514884495944,
      "loss": 16.6296,
      "step": 9140
    },
    {
      "epoch": 2.6558422731226443,
      "grad_norm": 9.128607749938965,
      "learning_rate": 0.00017951439851928287,
      "loss": 16.6375,
      "step": 9160
    },
    {
      "epoch": 2.6616410553783707,
      "grad_norm": 9.278364181518555,
      "learning_rate": 0.00017906337077226326,
      "loss": 16.4951,
      "step": 9180
    },
    {
      "epoch": 2.667439837634097,
      "grad_norm": 9.9072904586792,
      "learning_rate": 0.00017861206984334747,
      "loss": 16.5406,
      "step": 9200
    },
    {
      "epoch": 2.673238619889823,
      "grad_norm": 9.612322807312012,
      "learning_rate": 0.0001781604999745501,
      "loss": 16.4963,
      "step": 9220
    },
    {
      "epoch": 2.6790374021455494,
      "grad_norm": 9.36677074432373,
      "learning_rate": 0.0001777086654104137,
      "loss": 16.6302,
      "step": 9240
    },
    {
      "epoch": 2.684836184401276,
      "grad_norm": 9.201854705810547,
      "learning_rate": 0.00017725657039796878,
      "loss": 16.7038,
      "step": 9260
    },
    {
      "epoch": 2.690634966657002,
      "grad_norm": 9.45641040802002,
      "learning_rate": 0.0001768042191866939,
      "loss": 16.3889,
      "step": 9280
    },
    {
      "epoch": 2.696433748912728,
      "grad_norm": 9.141730308532715,
      "learning_rate": 0.00017635161602847594,
      "loss": 16.396,
      "step": 9300
    },
    {
      "epoch": 2.7022325311684545,
      "grad_norm": 9.030313491821289,
      "learning_rate": 0.0001758987651775698,
      "loss": 16.7126,
      "step": 9320
    },
    {
      "epoch": 2.708031313424181,
      "grad_norm": 8.93828296661377,
      "learning_rate": 0.0001754456708905586,
      "loss": 16.579,
      "step": 9340
    },
    {
      "epoch": 2.7138300956799073,
      "grad_norm": 9.542159080505371,
      "learning_rate": 0.00017499233742631368,
      "loss": 16.3282,
      "step": 9360
    },
    {
      "epoch": 2.7196288779356337,
      "grad_norm": 9.440520286560059,
      "learning_rate": 0.00017453876904595456,
      "loss": 16.4748,
      "step": 9380
    },
    {
      "epoch": 2.72542766019136,
      "grad_norm": 9.710092544555664,
      "learning_rate": 0.0001740849700128088,
      "loss": 16.401,
      "step": 9400
    },
    {
      "epoch": 2.731226442447086,
      "grad_norm": 9.809775352478027,
      "learning_rate": 0.00017363094459237196,
      "loss": 16.4268,
      "step": 9420
    },
    {
      "epoch": 2.7370252247028124,
      "grad_norm": 9.207795143127441,
      "learning_rate": 0.00017317669705226764,
      "loss": 16.3309,
      "step": 9440
    },
    {
      "epoch": 2.742824006958539,
      "grad_norm": 9.273736000061035,
      "learning_rate": 0.00017272223166220715,
      "loss": 16.3289,
      "step": 9460
    },
    {
      "epoch": 2.7486227892142647,
      "grad_norm": 9.677986145019531,
      "learning_rate": 0.0001722675526939496,
      "loss": 16.3111,
      "step": 9480
    },
    {
      "epoch": 2.754421571469991,
      "grad_norm": 8.870506286621094,
      "learning_rate": 0.0001718126644212615,
      "loss": 16.3129,
      "step": 9500
    },
    {
      "epoch": 2.7602203537257175,
      "grad_norm": 9.733453750610352,
      "learning_rate": 0.00017135757111987683,
      "loss": 16.2872,
      "step": 9520
    },
    {
      "epoch": 2.766019135981444,
      "grad_norm": 9.200676918029785,
      "learning_rate": 0.00017090227706745674,
      "loss": 16.2667,
      "step": 9540
    },
    {
      "epoch": 2.7718179182371703,
      "grad_norm": 9.284112930297852,
      "learning_rate": 0.00017044678654354922,
      "loss": 16.5073,
      "step": 9560
    },
    {
      "epoch": 2.7776167004928967,
      "grad_norm": 9.576952934265137,
      "learning_rate": 0.00016999110382954919,
      "loss": 16.2623,
      "step": 9580
    },
    {
      "epoch": 2.783415482748623,
      "grad_norm": 11.105751037597656,
      "learning_rate": 0.00016953523320865788,
      "loss": 16.1124,
      "step": 9600
    },
    {
      "epoch": 2.789214265004349,
      "grad_norm": 9.673952102661133,
      "learning_rate": 0.00016907917896584286,
      "loss": 16.3329,
      "step": 9620
    },
    {
      "epoch": 2.7950130472600754,
      "grad_norm": 9.506885528564453,
      "learning_rate": 0.00016862294538779767,
      "loss": 16.0656,
      "step": 9640
    },
    {
      "epoch": 2.8008118295158018,
      "grad_norm": 9.650944709777832,
      "learning_rate": 0.00016816653676290146,
      "loss": 16.2715,
      "step": 9660
    },
    {
      "epoch": 2.8066106117715277,
      "grad_norm": 9.401083946228027,
      "learning_rate": 0.00016770995738117877,
      "loss": 16.0092,
      "step": 9680
    },
    {
      "epoch": 2.812409394027254,
      "grad_norm": 8.969134330749512,
      "learning_rate": 0.00016725321153425912,
      "loss": 16.1556,
      "step": 9700
    },
    {
      "epoch": 2.8182081762829805,
      "grad_norm": 8.946815490722656,
      "learning_rate": 0.00016679630351533684,
      "loss": 16.1327,
      "step": 9720
    },
    {
      "epoch": 2.824006958538707,
      "grad_norm": 9.544264793395996,
      "learning_rate": 0.00016633923761913048,
      "loss": 16.1335,
      "step": 9740
    },
    {
      "epoch": 2.8298057407944333,
      "grad_norm": 8.524923324584961,
      "learning_rate": 0.00016588201814184266,
      "loss": 16.169,
      "step": 9760
    },
    {
      "epoch": 2.8356045230501596,
      "grad_norm": 9.184107780456543,
      "learning_rate": 0.00016542464938111952,
      "loss": 16.1466,
      "step": 9780
    },
    {
      "epoch": 2.841403305305886,
      "grad_norm": 8.591693878173828,
      "learning_rate": 0.00016496713563601037,
      "loss": 16.0429,
      "step": 9800
    },
    {
      "epoch": 2.847202087561612,
      "grad_norm": 9.47309684753418,
      "learning_rate": 0.00016450948120692735,
      "loss": 16.0873,
      "step": 9820
    },
    {
      "epoch": 2.8530008698173384,
      "grad_norm": 8.850638389587402,
      "learning_rate": 0.000164051690395605,
      "loss": 16.0723,
      "step": 9840
    },
    {
      "epoch": 2.8587996520730647,
      "grad_norm": 9.10097599029541,
      "learning_rate": 0.00016359376750505974,
      "loss": 15.91,
      "step": 9860
    },
    {
      "epoch": 2.8645984343287907,
      "grad_norm": 9.956915855407715,
      "learning_rate": 0.0001631357168395495,
      "loss": 16.0423,
      "step": 9880
    },
    {
      "epoch": 2.870397216584517,
      "grad_norm": 9.109548568725586,
      "learning_rate": 0.00016267754270453318,
      "loss": 15.9293,
      "step": 9900
    },
    {
      "epoch": 2.8761959988402435,
      "grad_norm": 9.158333778381348,
      "learning_rate": 0.00016221924940663024,
      "loss": 16.0816,
      "step": 9920
    },
    {
      "epoch": 2.88199478109597,
      "grad_norm": 8.739660263061523,
      "learning_rate": 0.00016176084125358033,
      "loss": 16.021,
      "step": 9940
    },
    {
      "epoch": 2.8877935633516962,
      "grad_norm": 9.015984535217285,
      "learning_rate": 0.00016130232255420253,
      "loss": 15.8741,
      "step": 9960
    },
    {
      "epoch": 2.8935923456074226,
      "grad_norm": 8.761491775512695,
      "learning_rate": 0.0001608436976183551,
      "loss": 15.8875,
      "step": 9980
    },
    {
      "epoch": 2.899391127863149,
      "grad_norm": 9.656839370727539,
      "learning_rate": 0.00016038497075689483,
      "loss": 15.9542,
      "step": 10000
    },
    {
      "epoch": 2.905189910118875,
      "grad_norm": 9.367596626281738,
      "learning_rate": 0.00015992614628163656,
      "loss": 15.9393,
      "step": 10020
    },
    {
      "epoch": 2.9109886923746013,
      "grad_norm": 8.683929443359375,
      "learning_rate": 0.00015946722850531273,
      "loss": 15.7471,
      "step": 10040
    },
    {
      "epoch": 2.9167874746303277,
      "grad_norm": 8.671602249145508,
      "learning_rate": 0.00015900822174153259,
      "loss": 15.8475,
      "step": 10060
    },
    {
      "epoch": 2.9225862568860537,
      "grad_norm": 9.880463600158691,
      "learning_rate": 0.00015854913030474206,
      "loss": 15.8583,
      "step": 10080
    },
    {
      "epoch": 2.92838503914178,
      "grad_norm": 8.80515193939209,
      "learning_rate": 0.00015808995851018277,
      "loss": 16.0011,
      "step": 10100
    },
    {
      "epoch": 2.9341838213975064,
      "grad_norm": 10.05087661743164,
      "learning_rate": 0.00015763071067385185,
      "loss": 15.9951,
      "step": 10120
    },
    {
      "epoch": 2.939982603653233,
      "grad_norm": 9.292128562927246,
      "learning_rate": 0.0001571713911124609,
      "loss": 15.788,
      "step": 10140
    },
    {
      "epoch": 2.945781385908959,
      "grad_norm": 8.959671020507812,
      "learning_rate": 0.00015671200414339593,
      "loss": 15.7954,
      "step": 10160
    },
    {
      "epoch": 2.9515801681646856,
      "grad_norm": 9.791890144348145,
      "learning_rate": 0.00015625255408467648,
      "loss": 15.7946,
      "step": 10180
    },
    {
      "epoch": 2.957378950420412,
      "grad_norm": 8.922806739807129,
      "learning_rate": 0.0001557930452549152,
      "loss": 15.7889,
      "step": 10200
    },
    {
      "epoch": 2.963177732676138,
      "grad_norm": 8.885083198547363,
      "learning_rate": 0.00015533348197327703,
      "loss": 15.6567,
      "step": 10220
    },
    {
      "epoch": 2.9689765149318643,
      "grad_norm": 8.869733810424805,
      "learning_rate": 0.0001548738685594388,
      "loss": 15.6325,
      "step": 10240
    },
    {
      "epoch": 2.9747752971875907,
      "grad_norm": 8.695548057556152,
      "learning_rate": 0.00015441420933354845,
      "loss": 15.5738,
      "step": 10260
    },
    {
      "epoch": 2.9805740794433166,
      "grad_norm": 9.313432693481445,
      "learning_rate": 0.00015395450861618472,
      "loss": 15.6212,
      "step": 10280
    },
    {
      "epoch": 2.986372861699043,
      "grad_norm": 9.230592727661133,
      "learning_rate": 0.0001534947707283162,
      "loss": 15.5188,
      "step": 10300
    },
    {
      "epoch": 2.9921716439547694,
      "grad_norm": 8.645792961120605,
      "learning_rate": 0.0001530349999912609,
      "loss": 15.3931,
      "step": 10320
    },
    {
      "epoch": 2.997970426210496,
      "grad_norm": 8.58164119720459,
      "learning_rate": 0.00015257520072664568,
      "loss": 15.5199,
      "step": 10340
    },
    {
      "epoch": 3.0,
      "eval_loss": 15.338022232055664,
      "eval_runtime": 178.5614,
      "eval_samples_per_second": 45.878,
      "eval_steps_per_second": 5.735,
      "step": 10347
    },
    {
      "epoch": 3.003769208466222,
      "grad_norm": 7.935316562652588,
      "learning_rate": 0.0001521153772563654,
      "loss": 15.2422,
      "step": 10360
    },
    {
      "epoch": 3.0095679907219486,
      "grad_norm": 8.200701713562012,
      "learning_rate": 0.00015165553390254254,
      "loss": 15.1135,
      "step": 10380
    },
    {
      "epoch": 3.0153667729776745,
      "grad_norm": 7.807314395904541,
      "learning_rate": 0.00015119567498748647,
      "loss": 15.115,
      "step": 10400
    },
    {
      "epoch": 3.021165555233401,
      "grad_norm": 9.131420135498047,
      "learning_rate": 0.00015073580483365275,
      "loss": 15.0969,
      "step": 10420
    },
    {
      "epoch": 3.0269643374891273,
      "grad_norm": 8.361710548400879,
      "learning_rate": 0.00015027592776360277,
      "loss": 15.0423,
      "step": 10440
    },
    {
      "epoch": 3.0327631197448537,
      "grad_norm": 8.558380126953125,
      "learning_rate": 0.00014981604809996275,
      "loss": 15.1618,
      "step": 10460
    },
    {
      "epoch": 3.03856190200058,
      "grad_norm": 8.479598045349121,
      "learning_rate": 0.00014935617016538326,
      "loss": 15.1168,
      "step": 10480
    },
    {
      "epoch": 3.044360684256306,
      "grad_norm": 8.256062507629395,
      "learning_rate": 0.00014889629828249872,
      "loss": 15.0624,
      "step": 10500
    },
    {
      "epoch": 3.0501594665120324,
      "grad_norm": 8.114568710327148,
      "learning_rate": 0.00014843643677388678,
      "loss": 15.0833,
      "step": 10520
    },
    {
      "epoch": 3.055958248767759,
      "grad_norm": 8.102224349975586,
      "learning_rate": 0.00014797658996202722,
      "loss": 15.1192,
      "step": 10540
    },
    {
      "epoch": 3.061757031023485,
      "grad_norm": 8.728391647338867,
      "learning_rate": 0.0001475167621692621,
      "loss": 15.1397,
      "step": 10560
    },
    {
      "epoch": 3.0675558132792116,
      "grad_norm": 8.121641159057617,
      "learning_rate": 0.00014705695771775434,
      "loss": 15.0775,
      "step": 10580
    },
    {
      "epoch": 3.0733545955349375,
      "grad_norm": 8.382612228393555,
      "learning_rate": 0.00014659718092944774,
      "loss": 14.8543,
      "step": 10600
    },
    {
      "epoch": 3.079153377790664,
      "grad_norm": 8.843401908874512,
      "learning_rate": 0.000146137436126026,
      "loss": 15.1723,
      "step": 10620
    },
    {
      "epoch": 3.0849521600463903,
      "grad_norm": 8.292612075805664,
      "learning_rate": 0.00014567772762887204,
      "loss": 15.0669,
      "step": 10640
    },
    {
      "epoch": 3.0907509423021167,
      "grad_norm": 8.062915802001953,
      "learning_rate": 0.0001452180597590277,
      "loss": 15.1008,
      "step": 10660
    },
    {
      "epoch": 3.096549724557843,
      "grad_norm": 8.374526977539062,
      "learning_rate": 0.0001447584368371528,
      "loss": 15.032,
      "step": 10680
    },
    {
      "epoch": 3.102348506813569,
      "grad_norm": 8.304373741149902,
      "learning_rate": 0.00014429886318348482,
      "loss": 15.0339,
      "step": 10700
    },
    {
      "epoch": 3.1081472890692954,
      "grad_norm": 8.052420616149902,
      "learning_rate": 0.00014383934311779797,
      "loss": 14.9794,
      "step": 10720
    },
    {
      "epoch": 3.1139460713250218,
      "grad_norm": 8.079130172729492,
      "learning_rate": 0.0001433798809593629,
      "loss": 15.076,
      "step": 10740
    },
    {
      "epoch": 3.119744853580748,
      "grad_norm": 9.176322937011719,
      "learning_rate": 0.00014292048102690596,
      "loss": 14.9177,
      "step": 10760
    },
    {
      "epoch": 3.1255436358364745,
      "grad_norm": 8.455704689025879,
      "learning_rate": 0.00014246114763856845,
      "loss": 14.9504,
      "step": 10780
    },
    {
      "epoch": 3.1313424180922005,
      "grad_norm": 7.990912914276123,
      "learning_rate": 0.0001420018851118664,
      "loss": 15.0326,
      "step": 10800
    },
    {
      "epoch": 3.137141200347927,
      "grad_norm": 7.820157527923584,
      "learning_rate": 0.00014154269776364952,
      "loss": 14.7523,
      "step": 10820
    },
    {
      "epoch": 3.1429399826036533,
      "grad_norm": 8.219060897827148,
      "learning_rate": 0.00014108358991006117,
      "loss": 15.0899,
      "step": 10840
    },
    {
      "epoch": 3.1487387648593796,
      "grad_norm": 8.4424409866333,
      "learning_rate": 0.0001406245658664973,
      "loss": 15.0402,
      "step": 10860
    },
    {
      "epoch": 3.154537547115106,
      "grad_norm": 7.462854862213135,
      "learning_rate": 0.00014016562994756607,
      "loss": 15.0928,
      "step": 10880
    },
    {
      "epoch": 3.160336329370832,
      "grad_norm": 8.410983085632324,
      "learning_rate": 0.00013970678646704749,
      "loss": 14.7499,
      "step": 10900
    },
    {
      "epoch": 3.1661351116265584,
      "grad_norm": 8.558433532714844,
      "learning_rate": 0.00013924803973785246,
      "loss": 14.9607,
      "step": 10920
    },
    {
      "epoch": 3.1719338938822847,
      "grad_norm": 8.835027694702148,
      "learning_rate": 0.0001387893940719827,
      "loss": 14.9637,
      "step": 10940
    },
    {
      "epoch": 3.177732676138011,
      "grad_norm": 8.242676734924316,
      "learning_rate": 0.00013833085378048976,
      "loss": 14.8754,
      "step": 10960
    },
    {
      "epoch": 3.1835314583937375,
      "grad_norm": 8.09783935546875,
      "learning_rate": 0.00013787242317343486,
      "loss": 14.9886,
      "step": 10980
    },
    {
      "epoch": 3.1893302406494635,
      "grad_norm": 8.194573402404785,
      "learning_rate": 0.00013741410655984828,
      "loss": 14.8514,
      "step": 11000
    },
    {
      "epoch": 3.19512902290519,
      "grad_norm": 7.7363810539245605,
      "learning_rate": 0.00013695590824768864,
      "loss": 14.8388,
      "step": 11020
    },
    {
      "epoch": 3.2009278051609162,
      "grad_norm": 8.160199165344238,
      "learning_rate": 0.00013649783254380274,
      "loss": 14.8631,
      "step": 11040
    },
    {
      "epoch": 3.2067265874166426,
      "grad_norm": 8.377543449401855,
      "learning_rate": 0.00013603988375388481,
      "loss": 15.0531,
      "step": 11060
    },
    {
      "epoch": 3.2125253696723686,
      "grad_norm": 8.218805313110352,
      "learning_rate": 0.00013558206618243624,
      "loss": 14.9043,
      "step": 11080
    },
    {
      "epoch": 3.218324151928095,
      "grad_norm": 8.988619804382324,
      "learning_rate": 0.00013512438413272506,
      "loss": 14.9641,
      "step": 11100
    },
    {
      "epoch": 3.2241229341838213,
      "grad_norm": 8.325322151184082,
      "learning_rate": 0.00013466684190674525,
      "loss": 14.9013,
      "step": 11120
    },
    {
      "epoch": 3.2299217164395477,
      "grad_norm": 8.390501022338867,
      "learning_rate": 0.00013420944380517676,
      "loss": 14.8531,
      "step": 11140
    },
    {
      "epoch": 3.235720498695274,
      "grad_norm": 8.110201835632324,
      "learning_rate": 0.00013375219412734465,
      "loss": 14.8536,
      "step": 11160
    },
    {
      "epoch": 3.2415192809510005,
      "grad_norm": 8.962289810180664,
      "learning_rate": 0.00013329509717117908,
      "loss": 14.7719,
      "step": 11180
    },
    {
      "epoch": 3.2473180632067264,
      "grad_norm": 7.933244228363037,
      "learning_rate": 0.0001328381572331744,
      "loss": 14.7414,
      "step": 11200
    },
    {
      "epoch": 3.253116845462453,
      "grad_norm": 8.457178115844727,
      "learning_rate": 0.00013238137860834933,
      "loss": 14.7615,
      "step": 11220
    },
    {
      "epoch": 3.258915627718179,
      "grad_norm": 8.602261543273926,
      "learning_rate": 0.00013192476559020628,
      "loss": 14.8374,
      "step": 11240
    },
    {
      "epoch": 3.2647144099739056,
      "grad_norm": 8.211438179016113,
      "learning_rate": 0.0001314683224706908,
      "loss": 14.8085,
      "step": 11260
    },
    {
      "epoch": 3.2705131922296315,
      "grad_norm": 8.524016380310059,
      "learning_rate": 0.0001310120535401519,
      "loss": 14.8621,
      "step": 11280
    },
    {
      "epoch": 3.276311974485358,
      "grad_norm": 7.6943254470825195,
      "learning_rate": 0.0001305559630873008,
      "loss": 14.6951,
      "step": 11300
    },
    {
      "epoch": 3.2821107567410843,
      "grad_norm": 7.714194297790527,
      "learning_rate": 0.00013010005539917152,
      "loss": 14.6946,
      "step": 11320
    },
    {
      "epoch": 3.2879095389968107,
      "grad_norm": 8.286245346069336,
      "learning_rate": 0.00012964433476108004,
      "loss": 14.774,
      "step": 11340
    },
    {
      "epoch": 3.293708321252537,
      "grad_norm": 7.858117580413818,
      "learning_rate": 0.00012918880545658408,
      "loss": 14.7255,
      "step": 11360
    },
    {
      "epoch": 3.2995071035082635,
      "grad_norm": 7.853677749633789,
      "learning_rate": 0.00012873347176744294,
      "loss": 14.6524,
      "step": 11380
    },
    {
      "epoch": 3.3053058857639894,
      "grad_norm": 8.08543586730957,
      "learning_rate": 0.00012827833797357735,
      "loss": 14.6309,
      "step": 11400
    },
    {
      "epoch": 3.311104668019716,
      "grad_norm": 8.833789825439453,
      "learning_rate": 0.00012782340835302902,
      "loss": 14.6779,
      "step": 11420
    },
    {
      "epoch": 3.316903450275442,
      "grad_norm": 7.768233776092529,
      "learning_rate": 0.00012736868718192046,
      "loss": 14.5867,
      "step": 11440
    },
    {
      "epoch": 3.3227022325311686,
      "grad_norm": 7.971062660217285,
      "learning_rate": 0.000126914178734415,
      "loss": 14.708,
      "step": 11460
    },
    {
      "epoch": 3.3285010147868945,
      "grad_norm": 8.544893264770508,
      "learning_rate": 0.00012645988728267647,
      "loss": 14.7052,
      "step": 11480
    },
    {
      "epoch": 3.334299797042621,
      "grad_norm": 7.973179340362549,
      "learning_rate": 0.00012600581709682883,
      "loss": 14.731,
      "step": 11500
    },
    {
      "epoch": 3.3400985792983473,
      "grad_norm": 8.309037208557129,
      "learning_rate": 0.00012555197244491656,
      "loss": 14.6987,
      "step": 11520
    },
    {
      "epoch": 3.3458973615540737,
      "grad_norm": 8.121941566467285,
      "learning_rate": 0.0001250983575928639,
      "loss": 14.5658,
      "step": 11540
    },
    {
      "epoch": 3.3516961438098,
      "grad_norm": 8.18394660949707,
      "learning_rate": 0.00012464497680443536,
      "loss": 14.5912,
      "step": 11560
    },
    {
      "epoch": 3.3574949260655265,
      "grad_norm": 8.151327133178711,
      "learning_rate": 0.00012419183434119523,
      "loss": 14.6985,
      "step": 11580
    },
    {
      "epoch": 3.3632937083212524,
      "grad_norm": 8.080724716186523,
      "learning_rate": 0.00012373893446246758,
      "loss": 14.7068,
      "step": 11600
    },
    {
      "epoch": 3.369092490576979,
      "grad_norm": 8.217824935913086,
      "learning_rate": 0.00012328628142529654,
      "loss": 14.4847,
      "step": 11620
    },
    {
      "epoch": 3.374891272832705,
      "grad_norm": 8.463308334350586,
      "learning_rate": 0.00012283387948440575,
      "loss": 14.4792,
      "step": 11640
    },
    {
      "epoch": 3.3806900550884316,
      "grad_norm": 7.818640232086182,
      "learning_rate": 0.00012238173289215893,
      "loss": 14.5756,
      "step": 11660
    },
    {
      "epoch": 3.3864888373441575,
      "grad_norm": 7.6987624168396,
      "learning_rate": 0.00012192984589851943,
      "loss": 14.5832,
      "step": 11680
    },
    {
      "epoch": 3.392287619599884,
      "grad_norm": 7.6923699378967285,
      "learning_rate": 0.00012147822275101068,
      "loss": 14.5558,
      "step": 11700
    },
    {
      "epoch": 3.3980864018556103,
      "grad_norm": 8.34443187713623,
      "learning_rate": 0.00012102686769467593,
      "loss": 14.6397,
      "step": 11720
    },
    {
      "epoch": 3.4038851841113367,
      "grad_norm": 8.113798141479492,
      "learning_rate": 0.0001205757849720386,
      "loss": 14.5117,
      "step": 11740
    },
    {
      "epoch": 3.409683966367063,
      "grad_norm": 8.578195571899414,
      "learning_rate": 0.00012012497882306231,
      "loss": 14.5703,
      "step": 11760
    },
    {
      "epoch": 3.4154827486227894,
      "grad_norm": 8.870187759399414,
      "learning_rate": 0.00011967445348511084,
      "loss": 14.5549,
      "step": 11780
    },
    {
      "epoch": 3.4212815308785154,
      "grad_norm": 8.046393394470215,
      "learning_rate": 0.0001192242131929087,
      "loss": 14.5884,
      "step": 11800
    },
    {
      "epoch": 3.4270803131342418,
      "grad_norm": 7.291285037994385,
      "learning_rate": 0.0001187742621785011,
      "loss": 14.5444,
      "step": 11820
    },
    {
      "epoch": 3.432879095389968,
      "grad_norm": 7.759986877441406,
      "learning_rate": 0.00011832460467121394,
      "loss": 14.462,
      "step": 11840
    },
    {
      "epoch": 3.4386778776456945,
      "grad_norm": 8.37353229522705,
      "learning_rate": 0.0001178752448976146,
      "loss": 14.6023,
      "step": 11860
    },
    {
      "epoch": 3.4444766599014205,
      "grad_norm": 8.234764099121094,
      "learning_rate": 0.0001174261870814716,
      "loss": 14.5913,
      "step": 11880
    },
    {
      "epoch": 3.450275442157147,
      "grad_norm": 8.630705833435059,
      "learning_rate": 0.00011697743544371552,
      "loss": 14.392,
      "step": 11900
    },
    {
      "epoch": 3.4560742244128733,
      "grad_norm": 8.065778732299805,
      "learning_rate": 0.00011652899420239872,
      "loss": 14.5397,
      "step": 11920
    },
    {
      "epoch": 3.4618730066685997,
      "grad_norm": 8.551312446594238,
      "learning_rate": 0.00011608086757265613,
      "loss": 14.4382,
      "step": 11940
    },
    {
      "epoch": 3.467671788924326,
      "grad_norm": 9.010847091674805,
      "learning_rate": 0.00011563305976666552,
      "loss": 14.4049,
      "step": 11960
    },
    {
      "epoch": 3.4734705711800524,
      "grad_norm": 8.020280838012695,
      "learning_rate": 0.0001151855749936077,
      "loss": 14.501,
      "step": 11980
    },
    {
      "epoch": 3.4792693534357784,
      "grad_norm": 8.101179122924805,
      "learning_rate": 0.00011473841745962736,
      "loss": 14.4839,
      "step": 12000
    },
    {
      "epoch": 3.4850681356915048,
      "grad_norm": 8.01736068725586,
      "learning_rate": 0.00011429159136779304,
      "loss": 14.4324,
      "step": 12020
    },
    {
      "epoch": 3.490866917947231,
      "grad_norm": 8.358084678649902,
      "learning_rate": 0.00011384510091805805,
      "loss": 14.5235,
      "step": 12040
    },
    {
      "epoch": 3.4966657002029575,
      "grad_norm": 8.307510375976562,
      "learning_rate": 0.00011339895030722086,
      "loss": 14.3874,
      "step": 12060
    },
    {
      "epoch": 3.5024644824586835,
      "grad_norm": 8.49990463256836,
      "learning_rate": 0.00011295314372888542,
      "loss": 14.5049,
      "step": 12080
    },
    {
      "epoch": 3.50826326471441,
      "grad_norm": 7.934238910675049,
      "learning_rate": 0.00011250768537342212,
      "loss": 14.5541,
      "step": 12100
    },
    {
      "epoch": 3.5140620469701362,
      "grad_norm": 7.56827449798584,
      "learning_rate": 0.00011206257942792812,
      "loss": 14.4541,
      "step": 12120
    },
    {
      "epoch": 3.5198608292258626,
      "grad_norm": 7.778404235839844,
      "learning_rate": 0.0001116178300761882,
      "loss": 14.427,
      "step": 12140
    },
    {
      "epoch": 3.525659611481589,
      "grad_norm": 8.210723876953125,
      "learning_rate": 0.00011117344149863514,
      "loss": 14.4185,
      "step": 12160
    },
    {
      "epoch": 3.5314583937373154,
      "grad_norm": 9.81620979309082,
      "learning_rate": 0.00011072941787231077,
      "loss": 14.5096,
      "step": 12180
    },
    {
      "epoch": 3.5372571759930413,
      "grad_norm": 8.085357666015625,
      "learning_rate": 0.00011028576337082661,
      "loss": 14.2829,
      "step": 12200
    },
    {
      "epoch": 3.5430559582487677,
      "grad_norm": 7.475404739379883,
      "learning_rate": 0.00010984248216432433,
      "loss": 14.3374,
      "step": 12220
    },
    {
      "epoch": 3.548854740504494,
      "grad_norm": 8.399643898010254,
      "learning_rate": 0.0001093995784194371,
      "loss": 14.4408,
      "step": 12240
    },
    {
      "epoch": 3.5546535227602205,
      "grad_norm": 8.295843124389648,
      "learning_rate": 0.00010895705629924988,
      "loss": 14.2995,
      "step": 12260
    },
    {
      "epoch": 3.5604523050159465,
      "grad_norm": 7.9457268714904785,
      "learning_rate": 0.00010851491996326067,
      "loss": 14.291,
      "step": 12280
    },
    {
      "epoch": 3.566251087271673,
      "grad_norm": 7.893642425537109,
      "learning_rate": 0.00010807317356734132,
      "loss": 14.2564,
      "step": 12300
    },
    {
      "epoch": 3.5720498695273992,
      "grad_norm": 7.6671600341796875,
      "learning_rate": 0.00010763182126369824,
      "loss": 14.2539,
      "step": 12320
    },
    {
      "epoch": 3.5778486517831256,
      "grad_norm": 8.033576011657715,
      "learning_rate": 0.00010719086720083386,
      "loss": 14.2711,
      "step": 12340
    },
    {
      "epoch": 3.583647434038852,
      "grad_norm": 7.441287040710449,
      "learning_rate": 0.00010675031552350699,
      "loss": 14.2639,
      "step": 12360
    },
    {
      "epoch": 3.5894462162945784,
      "grad_norm": 8.565853118896484,
      "learning_rate": 0.00010631017037269449,
      "loss": 14.3288,
      "step": 12380
    },
    {
      "epoch": 3.5952449985503043,
      "grad_norm": 8.45598316192627,
      "learning_rate": 0.00010587043588555187,
      "loss": 14.2665,
      "step": 12400
    },
    {
      "epoch": 3.6010437808060307,
      "grad_norm": 8.051493644714355,
      "learning_rate": 0.00010543111619537473,
      "loss": 14.2565,
      "step": 12420
    },
    {
      "epoch": 3.606842563061757,
      "grad_norm": 7.820382118225098,
      "learning_rate": 0.00010499221543155974,
      "loss": 14.1093,
      "step": 12440
    },
    {
      "epoch": 3.6126413453174835,
      "grad_norm": 7.687190055847168,
      "learning_rate": 0.0001045537377195658,
      "loss": 14.2915,
      "step": 12460
    },
    {
      "epoch": 3.6184401275732094,
      "grad_norm": 8.168779373168945,
      "learning_rate": 0.00010411568718087542,
      "loss": 14.3527,
      "step": 12480
    },
    {
      "epoch": 3.624238909828936,
      "grad_norm": 7.903017997741699,
      "learning_rate": 0.00010367806793295575,
      "loss": 14.2181,
      "step": 12500
    },
    {
      "epoch": 3.630037692084662,
      "grad_norm": 7.881480693817139,
      "learning_rate": 0.00010324088408922011,
      "loss": 14.3049,
      "step": 12520
    },
    {
      "epoch": 3.6358364743403886,
      "grad_norm": 7.166464328765869,
      "learning_rate": 0.00010280413975898933,
      "loss": 14.1224,
      "step": 12540
    },
    {
      "epoch": 3.641635256596115,
      "grad_norm": 7.990483283996582,
      "learning_rate": 0.00010236783904745274,
      "loss": 14.1902,
      "step": 12560
    },
    {
      "epoch": 3.6474340388518414,
      "grad_norm": 8.040324211120605,
      "learning_rate": 0.00010193198605563014,
      "loss": 14.2086,
      "step": 12580
    },
    {
      "epoch": 3.6532328211075673,
      "grad_norm": 7.55288028717041,
      "learning_rate": 0.00010149658488033277,
      "loss": 14.1929,
      "step": 12600
    },
    {
      "epoch": 3.6590316033632937,
      "grad_norm": 8.312182426452637,
      "learning_rate": 0.00010106163961412523,
      "loss": 14.1696,
      "step": 12620
    },
    {
      "epoch": 3.66483038561902,
      "grad_norm": 7.902687072753906,
      "learning_rate": 0.00010062715434528654,
      "loss": 14.1201,
      "step": 12640
    },
    {
      "epoch": 3.6706291678747465,
      "grad_norm": 8.263748168945312,
      "learning_rate": 0.00010019313315777212,
      "loss": 14.0304,
      "step": 12660
    },
    {
      "epoch": 3.6764279501304724,
      "grad_norm": 9.446555137634277,
      "learning_rate": 9.975958013117525e-05,
      "loss": 14.0939,
      "step": 12680
    },
    {
      "epoch": 3.682226732386199,
      "grad_norm": 7.74048376083374,
      "learning_rate": 9.932649934068862e-05,
      "loss": 14.0794,
      "step": 12700
    },
    {
      "epoch": 3.688025514641925,
      "grad_norm": 7.776381015777588,
      "learning_rate": 9.889389485706626e-05,
      "loss": 14.1639,
      "step": 12720
    },
    {
      "epoch": 3.6938242968976516,
      "grad_norm": 7.615856647491455,
      "learning_rate": 9.846177074658498e-05,
      "loss": 14.086,
      "step": 12740
    },
    {
      "epoch": 3.699623079153378,
      "grad_norm": 7.816654682159424,
      "learning_rate": 9.803013107100643e-05,
      "loss": 14.2311,
      "step": 12760
    },
    {
      "epoch": 3.7054218614091043,
      "grad_norm": 7.591334342956543,
      "learning_rate": 9.759897988753888e-05,
      "loss": 14.1741,
      "step": 12780
    },
    {
      "epoch": 3.7112206436648303,
      "grad_norm": 7.784627914428711,
      "learning_rate": 9.716832124879876e-05,
      "loss": 14.173,
      "step": 12800
    },
    {
      "epoch": 3.7170194259205567,
      "grad_norm": 8.223350524902344,
      "learning_rate": 9.673815920277298e-05,
      "loss": 14.1514,
      "step": 12820
    },
    {
      "epoch": 3.722818208176283,
      "grad_norm": 7.568990707397461,
      "learning_rate": 9.63084977927807e-05,
      "loss": 14.1575,
      "step": 12840
    },
    {
      "epoch": 3.728616990432009,
      "grad_norm": 7.288310527801514,
      "learning_rate": 9.587934105743534e-05,
      "loss": 14.1393,
      "step": 12860
    },
    {
      "epoch": 3.7344157726877354,
      "grad_norm": 8.09030818939209,
      "learning_rate": 9.545069303060652e-05,
      "loss": 13.9691,
      "step": 12880
    },
    {
      "epoch": 3.7402145549434618,
      "grad_norm": 7.881587982177734,
      "learning_rate": 9.502255774138237e-05,
      "loss": 14.0598,
      "step": 12900
    },
    {
      "epoch": 3.746013337199188,
      "grad_norm": 7.82423734664917,
      "learning_rate": 9.459493921403156e-05,
      "loss": 14.0186,
      "step": 12920
    },
    {
      "epoch": 3.7518121194549146,
      "grad_norm": 9.032488822937012,
      "learning_rate": 9.41678414679652e-05,
      "loss": 14.1191,
      "step": 12940
    },
    {
      "epoch": 3.757610901710641,
      "grad_norm": 8.11778736114502,
      "learning_rate": 9.374126851769963e-05,
      "loss": 14.026,
      "step": 12960
    },
    {
      "epoch": 3.7634096839663673,
      "grad_norm": 7.4647674560546875,
      "learning_rate": 9.331522437281812e-05,
      "loss": 13.9676,
      "step": 12980
    },
    {
      "epoch": 3.7692084662220933,
      "grad_norm": 7.929332256317139,
      "learning_rate": 9.288971303793345e-05,
      "loss": 14.0064,
      "step": 13000
    },
    {
      "epoch": 3.7750072484778197,
      "grad_norm": 9.225380897521973,
      "learning_rate": 9.24647385126504e-05,
      "loss": 14.0039,
      "step": 13020
    },
    {
      "epoch": 3.780806030733546,
      "grad_norm": 9.065258026123047,
      "learning_rate": 9.204030479152773e-05,
      "loss": 14.0735,
      "step": 13040
    },
    {
      "epoch": 3.786604812989272,
      "grad_norm": 7.578149795532227,
      "learning_rate": 9.16164158640412e-05,
      "loss": 14.0243,
      "step": 13060
    },
    {
      "epoch": 3.7924035952449984,
      "grad_norm": 7.9267578125,
      "learning_rate": 9.119307571454549e-05,
      "loss": 13.995,
      "step": 13080
    },
    {
      "epoch": 3.7982023775007248,
      "grad_norm": 8.259258270263672,
      "learning_rate": 9.077028832223727e-05,
      "loss": 13.9401,
      "step": 13100
    },
    {
      "epoch": 3.804001159756451,
      "grad_norm": 7.756104946136475,
      "learning_rate": 9.03480576611173e-05,
      "loss": 13.9895,
      "step": 13120
    },
    {
      "epoch": 3.8097999420121775,
      "grad_norm": 7.989884853363037,
      "learning_rate": 8.994745782017989e-05,
      "loss": 14.1586,
      "step": 13140
    },
    {
      "epoch": 3.815598724267904,
      "grad_norm": 8.126604080200195,
      "learning_rate": 8.952632419524595e-05,
      "loss": 13.9488,
      "step": 13160
    },
    {
      "epoch": 3.8213975065236303,
      "grad_norm": 7.832819938659668,
      "learning_rate": 8.910575899417295e-05,
      "loss": 13.9588,
      "step": 13180
    },
    {
      "epoch": 3.8271962887793562,
      "grad_norm": 8.145319938659668,
      "learning_rate": 8.868576617007425e-05,
      "loss": 13.9174,
      "step": 13200
    },
    {
      "epoch": 3.8329950710350826,
      "grad_norm": 7.643582344055176,
      "learning_rate": 8.826634967068306e-05,
      "loss": 13.8962,
      "step": 13220
    },
    {
      "epoch": 3.838793853290809,
      "grad_norm": 7.718532562255859,
      "learning_rate": 8.784751343831559e-05,
      "loss": 13.8623,
      "step": 13240
    },
    {
      "epoch": 3.844592635546535,
      "grad_norm": 8.872835159301758,
      "learning_rate": 8.742926140983358e-05,
      "loss": 13.9224,
      "step": 13260
    },
    {
      "epoch": 3.8503914178022614,
      "grad_norm": 7.050908088684082,
      "learning_rate": 8.701159751660775e-05,
      "loss": 13.9465,
      "step": 13280
    },
    {
      "epoch": 3.8561902000579877,
      "grad_norm": 7.956148624420166,
      "learning_rate": 8.659452568448052e-05,
      "loss": 13.8704,
      "step": 13300
    },
    {
      "epoch": 3.861988982313714,
      "grad_norm": 7.912492752075195,
      "learning_rate": 8.617804983372923e-05,
      "loss": 13.7985,
      "step": 13320
    },
    {
      "epoch": 3.8677877645694405,
      "grad_norm": 8.003410339355469,
      "learning_rate": 8.576217387902927e-05,
      "loss": 13.9504,
      "step": 13340
    },
    {
      "epoch": 3.873586546825167,
      "grad_norm": 7.805898189544678,
      "learning_rate": 8.53469017294173e-05,
      "loss": 13.875,
      "step": 13360
    },
    {
      "epoch": 3.8793853290808933,
      "grad_norm": 7.951571464538574,
      "learning_rate": 8.49322372882545e-05,
      "loss": 13.8312,
      "step": 13380
    },
    {
      "epoch": 3.8851841113366192,
      "grad_norm": 7.454975128173828,
      "learning_rate": 8.451818445318988e-05,
      "loss": 13.9182,
      "step": 13400
    },
    {
      "epoch": 3.8909828935923456,
      "grad_norm": 7.393859386444092,
      "learning_rate": 8.410474711612368e-05,
      "loss": 13.7694,
      "step": 13420
    },
    {
      "epoch": 3.896781675848072,
      "grad_norm": 7.831262588500977,
      "learning_rate": 8.369192916317068e-05,
      "loss": 13.8574,
      "step": 13440
    },
    {
      "epoch": 3.902580458103798,
      "grad_norm": 8.03665542602539,
      "learning_rate": 8.327973447462377e-05,
      "loss": 13.9063,
      "step": 13460
    },
    {
      "epoch": 3.9083792403595243,
      "grad_norm": 7.440769195556641,
      "learning_rate": 8.286816692491747e-05,
      "loss": 13.6986,
      "step": 13480
    },
    {
      "epoch": 3.9141780226152507,
      "grad_norm": 7.012092590332031,
      "learning_rate": 8.245723038259148e-05,
      "loss": 13.7276,
      "step": 13500
    },
    {
      "epoch": 3.919976804870977,
      "grad_norm": 8.024231910705566,
      "learning_rate": 8.204692871025432e-05,
      "loss": 13.7756,
      "step": 13520
    },
    {
      "epoch": 3.9257755871267035,
      "grad_norm": 7.533725738525391,
      "learning_rate": 8.163726576454707e-05,
      "loss": 13.7441,
      "step": 13540
    },
    {
      "epoch": 3.93157436938243,
      "grad_norm": 7.5401082038879395,
      "learning_rate": 8.122824539610702e-05,
      "loss": 13.7335,
      "step": 13560
    },
    {
      "epoch": 3.937373151638156,
      "grad_norm": 7.167696475982666,
      "learning_rate": 8.081987144953158e-05,
      "loss": 13.857,
      "step": 13580
    },
    {
      "epoch": 3.943171933893882,
      "grad_norm": 8.670721054077148,
      "learning_rate": 8.041214776334213e-05,
      "loss": 13.7925,
      "step": 13600
    },
    {
      "epoch": 3.9489707161496086,
      "grad_norm": 7.68991231918335,
      "learning_rate": 8.000507816994785e-05,
      "loss": 13.8268,
      "step": 13620
    },
    {
      "epoch": 3.954769498405335,
      "grad_norm": 7.845467567443848,
      "learning_rate": 7.959866649560978e-05,
      "loss": 13.7864,
      "step": 13640
    },
    {
      "epoch": 3.960568280661061,
      "grad_norm": 8.191815376281738,
      "learning_rate": 7.919291656040479e-05,
      "loss": 13.7013,
      "step": 13660
    },
    {
      "epoch": 3.9663670629167873,
      "grad_norm": 7.444506645202637,
      "learning_rate": 7.878783217818991e-05,
      "loss": 13.568,
      "step": 13680
    },
    {
      "epoch": 3.9721658451725137,
      "grad_norm": 7.571342468261719,
      "learning_rate": 7.838341715656603e-05,
      "loss": 13.6824,
      "step": 13700
    },
    {
      "epoch": 3.97796462742824,
      "grad_norm": 7.676664352416992,
      "learning_rate": 7.797967529684247e-05,
      "loss": 13.7032,
      "step": 13720
    },
    {
      "epoch": 3.9837634096839665,
      "grad_norm": 7.022433757781982,
      "learning_rate": 7.757661039400119e-05,
      "loss": 13.7704,
      "step": 13740
    },
    {
      "epoch": 3.989562191939693,
      "grad_norm": 7.17979621887207,
      "learning_rate": 7.717422623666099e-05,
      "loss": 13.758,
      "step": 13760
    },
    {
      "epoch": 3.995360974195419,
      "grad_norm": 8.162264823913574,
      "learning_rate": 7.677252660704218e-05,
      "loss": 13.7213,
      "step": 13780
    },
    {
      "epoch": 4.0,
      "eval_loss": 13.832204818725586,
      "eval_runtime": 179.0425,
      "eval_samples_per_second": 45.754,
      "eval_steps_per_second": 5.719,
      "step": 13796
    },
    {
      "epoch": 4.001159756451146,
      "grad_norm": 7.502448558807373,
      "learning_rate": 7.63715152809305e-05,
      "loss": 13.6885,
      "step": 13800
    },
    {
      "epoch": 4.006958538706871,
      "grad_norm": 7.304751396179199,
      "learning_rate": 7.597119602764238e-05,
      "loss": 13.391,
      "step": 13820
    },
    {
      "epoch": 4.0127573209625975,
      "grad_norm": 8.74656867980957,
      "learning_rate": 7.557157260998861e-05,
      "loss": 13.3495,
      "step": 13840
    },
    {
      "epoch": 4.018556103218324,
      "grad_norm": 6.97358512878418,
      "learning_rate": 7.517264878423999e-05,
      "loss": 13.5563,
      "step": 13860
    },
    {
      "epoch": 4.02435488547405,
      "grad_norm": 7.195950508117676,
      "learning_rate": 7.477442830009094e-05,
      "loss": 13.51,
      "step": 13880
    },
    {
      "epoch": 4.030153667729777,
      "grad_norm": 7.479466915130615,
      "learning_rate": 7.437691490062526e-05,
      "loss": 13.4375,
      "step": 13900
    },
    {
      "epoch": 4.035952449985503,
      "grad_norm": 7.257887840270996,
      "learning_rate": 7.398011232228032e-05,
      "loss": 13.4411,
      "step": 13920
    },
    {
      "epoch": 4.0417512322412295,
      "grad_norm": 7.074364185333252,
      "learning_rate": 7.358402429481192e-05,
      "loss": 13.3696,
      "step": 13940
    },
    {
      "epoch": 4.047550014496956,
      "grad_norm": 7.195831298828125,
      "learning_rate": 7.31886545412598e-05,
      "loss": 13.5066,
      "step": 13960
    },
    {
      "epoch": 4.053348796752682,
      "grad_norm": 8.05786418914795,
      "learning_rate": 7.279400677791189e-05,
      "loss": 13.4364,
      "step": 13980
    },
    {
      "epoch": 4.059147579008409,
      "grad_norm": 7.219696044921875,
      "learning_rate": 7.240008471427001e-05,
      "loss": 13.3577,
      "step": 14000
    },
    {
      "epoch": 4.064946361264134,
      "grad_norm": 7.010767459869385,
      "learning_rate": 7.200689205301467e-05,
      "loss": 13.3087,
      "step": 14020
    },
    {
      "epoch": 4.0707451435198605,
      "grad_norm": 7.301575183868408,
      "learning_rate": 7.161443248997033e-05,
      "loss": 13.4342,
      "step": 14040
    },
    {
      "epoch": 4.076543925775587,
      "grad_norm": 7.263577938079834,
      "learning_rate": 7.122270971407065e-05,
      "loss": 13.4294,
      "step": 14060
    },
    {
      "epoch": 4.082342708031313,
      "grad_norm": 7.471279621124268,
      "learning_rate": 7.08317274073239e-05,
      "loss": 13.2503,
      "step": 14080
    },
    {
      "epoch": 4.08814149028704,
      "grad_norm": 7.099451541900635,
      "learning_rate": 7.044148924477826e-05,
      "loss": 13.4536,
      "step": 14100
    },
    {
      "epoch": 4.093940272542766,
      "grad_norm": 6.944591045379639,
      "learning_rate": 7.005199889448729e-05,
      "loss": 13.3969,
      "step": 14120
    },
    {
      "epoch": 4.099739054798492,
      "grad_norm": 7.546153545379639,
      "learning_rate": 6.966326001747551e-05,
      "loss": 13.4027,
      "step": 14140
    },
    {
      "epoch": 4.105537837054219,
      "grad_norm": 6.919654846191406,
      "learning_rate": 6.927527626770392e-05,
      "loss": 13.4429,
      "step": 14160
    },
    {
      "epoch": 4.111336619309945,
      "grad_norm": 7.007724761962891,
      "learning_rate": 6.888805129203569e-05,
      "loss": 13.4722,
      "step": 14180
    },
    {
      "epoch": 4.117135401565672,
      "grad_norm": 6.938448905944824,
      "learning_rate": 6.850158873020188e-05,
      "loss": 13.3654,
      "step": 14200
    },
    {
      "epoch": 4.122934183821397,
      "grad_norm": 7.060428142547607,
      "learning_rate": 6.811589221476719e-05,
      "loss": 13.2951,
      "step": 14220
    },
    {
      "epoch": 4.1287329660771235,
      "grad_norm": 7.970691204071045,
      "learning_rate": 6.773096537109589e-05,
      "loss": 13.4541,
      "step": 14240
    },
    {
      "epoch": 4.13453174833285,
      "grad_norm": 7.14845609664917,
      "learning_rate": 6.734681181731768e-05,
      "loss": 13.3372,
      "step": 14260
    },
    {
      "epoch": 4.140330530588576,
      "grad_norm": 7.267126083374023,
      "learning_rate": 6.696343516429372e-05,
      "loss": 13.293,
      "step": 14280
    },
    {
      "epoch": 4.146129312844303,
      "grad_norm": 7.2398295402526855,
      "learning_rate": 6.658083901558263e-05,
      "loss": 13.3337,
      "step": 14300
    },
    {
      "epoch": 4.151928095100029,
      "grad_norm": 7.114429473876953,
      "learning_rate": 6.619902696740671e-05,
      "loss": 13.4059,
      "step": 14320
    },
    {
      "epoch": 4.157726877355755,
      "grad_norm": 6.982402324676514,
      "learning_rate": 6.581800260861805e-05,
      "loss": 13.3159,
      "step": 14340
    },
    {
      "epoch": 4.163525659611482,
      "grad_norm": 7.156046390533447,
      "learning_rate": 6.543776952066483e-05,
      "loss": 13.329,
      "step": 14360
    },
    {
      "epoch": 4.169324441867208,
      "grad_norm": 6.745576858520508,
      "learning_rate": 6.505833127755765e-05,
      "loss": 13.2374,
      "step": 14380
    },
    {
      "epoch": 4.175123224122935,
      "grad_norm": 6.806048393249512,
      "learning_rate": 6.467969144583612e-05,
      "loss": 13.3605,
      "step": 14400
    },
    {
      "epoch": 4.18092200637866,
      "grad_norm": 7.469301223754883,
      "learning_rate": 6.430185358453485e-05,
      "loss": 13.3494,
      "step": 14420
    },
    {
      "epoch": 4.1867207886343865,
      "grad_norm": 6.763400077819824,
      "learning_rate": 6.392482124515053e-05,
      "loss": 13.239,
      "step": 14440
    },
    {
      "epoch": 4.192519570890113,
      "grad_norm": 7.1692023277282715,
      "learning_rate": 6.354859797160826e-05,
      "loss": 13.2073,
      "step": 14460
    },
    {
      "epoch": 4.198318353145839,
      "grad_norm": 7.078808307647705,
      "learning_rate": 6.317318730022828e-05,
      "loss": 13.3534,
      "step": 14480
    },
    {
      "epoch": 4.204117135401566,
      "grad_norm": 7.0460309982299805,
      "learning_rate": 6.279859275969292e-05,
      "loss": 13.363,
      "step": 14500
    },
    {
      "epoch": 4.209915917657292,
      "grad_norm": 6.922562599182129,
      "learning_rate": 6.242481787101294e-05,
      "loss": 13.2421,
      "step": 14520
    },
    {
      "epoch": 4.215714699913018,
      "grad_norm": 6.575055122375488,
      "learning_rate": 6.205186614749515e-05,
      "loss": 13.211,
      "step": 14540
    },
    {
      "epoch": 4.221513482168745,
      "grad_norm": 6.927389144897461,
      "learning_rate": 6.167974109470862e-05,
      "loss": 13.2338,
      "step": 14560
    },
    {
      "epoch": 4.227312264424471,
      "grad_norm": 7.155137062072754,
      "learning_rate": 6.130844621045252e-05,
      "loss": 13.317,
      "step": 14580
    },
    {
      "epoch": 4.2331110466801976,
      "grad_norm": 7.168717384338379,
      "learning_rate": 6.093798498472231e-05,
      "loss": 13.2903,
      "step": 14600
    },
    {
      "epoch": 4.238909828935923,
      "grad_norm": 7.363083839416504,
      "learning_rate": 6.056836089967798e-05,
      "loss": 13.3428,
      "step": 14620
    },
    {
      "epoch": 4.244708611191649,
      "grad_norm": 6.885748386383057,
      "learning_rate": 6.0199577429610546e-05,
      "loss": 13.1554,
      "step": 14640
    },
    {
      "epoch": 4.250507393447376,
      "grad_norm": 7.144756317138672,
      "learning_rate": 5.983163804090948e-05,
      "loss": 13.2239,
      "step": 14660
    },
    {
      "epoch": 4.256306175703102,
      "grad_norm": 7.061102867126465,
      "learning_rate": 5.94645461920307e-05,
      "loss": 13.2535,
      "step": 14680
    },
    {
      "epoch": 4.262104957958829,
      "grad_norm": 6.836531639099121,
      "learning_rate": 5.90983053334632e-05,
      "loss": 13.2416,
      "step": 14700
    },
    {
      "epoch": 4.267903740214555,
      "grad_norm": 7.061801433563232,
      "learning_rate": 5.873291890769744e-05,
      "loss": 13.2762,
      "step": 14720
    },
    {
      "epoch": 4.273702522470281,
      "grad_norm": 7.64084529876709,
      "learning_rate": 5.836839034919242e-05,
      "loss": 13.1391,
      "step": 14740
    },
    {
      "epoch": 4.279501304726008,
      "grad_norm": 7.055532455444336,
      "learning_rate": 5.800472308434363e-05,
      "loss": 13.1409,
      "step": 14760
    },
    {
      "epoch": 4.285300086981734,
      "grad_norm": 7.149598598480225,
      "learning_rate": 5.764192053145083e-05,
      "loss": 13.2142,
      "step": 14780
    },
    {
      "epoch": 4.2910988692374605,
      "grad_norm": 7.7407917976379395,
      "learning_rate": 5.727998610068588e-05,
      "loss": 13.0897,
      "step": 14800
    },
    {
      "epoch": 4.296897651493186,
      "grad_norm": 7.515734672546387,
      "learning_rate": 5.6918923194060775e-05,
      "loss": 13.1564,
      "step": 14820
    },
    {
      "epoch": 4.302696433748912,
      "grad_norm": 7.034214019775391,
      "learning_rate": 5.655873520539536e-05,
      "loss": 13.1367,
      "step": 14840
    },
    {
      "epoch": 4.308495216004639,
      "grad_norm": 7.0785813331604,
      "learning_rate": 5.619942552028599e-05,
      "loss": 13.2262,
      "step": 14860
    },
    {
      "epoch": 4.314293998260365,
      "grad_norm": 7.581841468811035,
      "learning_rate": 5.58409975160732e-05,
      "loss": 13.1744,
      "step": 14880
    },
    {
      "epoch": 4.320092780516092,
      "grad_norm": 7.300427436828613,
      "learning_rate": 5.548345456181014e-05,
      "loss": 13.149,
      "step": 14900
    },
    {
      "epoch": 4.325891562771818,
      "grad_norm": 7.163527488708496,
      "learning_rate": 5.512680001823099e-05,
      "loss": 13.2336,
      "step": 14920
    },
    {
      "epoch": 4.331690345027544,
      "grad_norm": 6.851948261260986,
      "learning_rate": 5.477103723771921e-05,
      "loss": 13.0083,
      "step": 14940
    },
    {
      "epoch": 4.337489127283271,
      "grad_norm": 7.069833278656006,
      "learning_rate": 5.4416169564276156e-05,
      "loss": 13.1516,
      "step": 14960
    },
    {
      "epoch": 4.343287909538997,
      "grad_norm": 6.89900541305542,
      "learning_rate": 5.4062200333489596e-05,
      "loss": 13.0811,
      "step": 14980
    },
    {
      "epoch": 4.3490866917947235,
      "grad_norm": 7.412498474121094,
      "learning_rate": 5.370913287250235e-05,
      "loss": 13.1326,
      "step": 15000
    },
    {
      "epoch": 4.354885474050449,
      "grad_norm": 6.785919189453125,
      "learning_rate": 5.335697049998105e-05,
      "loss": 13.0798,
      "step": 15020
    },
    {
      "epoch": 4.360684256306175,
      "grad_norm": 7.460022449493408,
      "learning_rate": 5.30057165260849e-05,
      "loss": 13.0705,
      "step": 15040
    },
    {
      "epoch": 4.366483038561902,
      "grad_norm": 6.917072772979736,
      "learning_rate": 5.2655374252434615e-05,
      "loss": 13.1399,
      "step": 15060
    },
    {
      "epoch": 4.372281820817628,
      "grad_norm": 9.193827629089355,
      "learning_rate": 5.2305946972081366e-05,
      "loss": 13.0694,
      "step": 15080
    },
    {
      "epoch": 4.378080603073355,
      "grad_norm": 7.091933250427246,
      "learning_rate": 5.19574379694757e-05,
      "loss": 13.0747,
      "step": 15100
    },
    {
      "epoch": 4.383879385329081,
      "grad_norm": 7.027653217315674,
      "learning_rate": 5.16098505204371e-05,
      "loss": 13.08,
      "step": 15120
    },
    {
      "epoch": 4.389678167584807,
      "grad_norm": 6.704803943634033,
      "learning_rate": 5.1263187892122495e-05,
      "loss": 13.0866,
      "step": 15140
    },
    {
      "epoch": 4.395476949840534,
      "grad_norm": 7.110939025878906,
      "learning_rate": 5.091745334299616e-05,
      "loss": 13.085,
      "step": 15160
    },
    {
      "epoch": 4.40127573209626,
      "grad_norm": 6.969048500061035,
      "learning_rate": 5.0572650122798847e-05,
      "loss": 13.0279,
      "step": 15180
    },
    {
      "epoch": 4.4070745143519865,
      "grad_norm": 7.2398762702941895,
      "learning_rate": 5.022878147251716e-05,
      "loss": 12.9762,
      "step": 15200
    },
    {
      "epoch": 4.412873296607712,
      "grad_norm": 7.724974155426025,
      "learning_rate": 4.988585062435343e-05,
      "loss": 13.1142,
      "step": 15220
    },
    {
      "epoch": 4.418672078863438,
      "grad_norm": 6.956549644470215,
      "learning_rate": 4.954386080169473e-05,
      "loss": 13.0925,
      "step": 15240
    },
    {
      "epoch": 4.424470861119165,
      "grad_norm": 7.126942157745361,
      "learning_rate": 4.920281521908333e-05,
      "loss": 12.9101,
      "step": 15260
    },
    {
      "epoch": 4.430269643374891,
      "grad_norm": 7.030598163604736,
      "learning_rate": 4.8862717082185685e-05,
      "loss": 13.0738,
      "step": 15280
    },
    {
      "epoch": 4.4360684256306175,
      "grad_norm": 7.476264476776123,
      "learning_rate": 4.852356958776314e-05,
      "loss": 13.1047,
      "step": 15300
    },
    {
      "epoch": 4.441867207886344,
      "grad_norm": 6.874753475189209,
      "learning_rate": 4.818537592364103e-05,
      "loss": 12.9725,
      "step": 15320
    },
    {
      "epoch": 4.44766599014207,
      "grad_norm": 7.013265132904053,
      "learning_rate": 4.7848139268679495e-05,
      "loss": 12.989,
      "step": 15340
    },
    {
      "epoch": 4.453464772397797,
      "grad_norm": 7.3559699058532715,
      "learning_rate": 4.75118627927431e-05,
      "loss": 13.0453,
      "step": 15360
    },
    {
      "epoch": 4.459263554653523,
      "grad_norm": 7.07127571105957,
      "learning_rate": 4.7176549656671e-05,
      "loss": 13.067,
      "step": 15380
    },
    {
      "epoch": 4.465062336909249,
      "grad_norm": 7.07823371887207,
      "learning_rate": 4.6842203012247776e-05,
      "loss": 13.0324,
      "step": 15400
    },
    {
      "epoch": 4.470861119164975,
      "grad_norm": 7.51452112197876,
      "learning_rate": 4.65088260021731e-05,
      "loss": 12.9859,
      "step": 15420
    },
    {
      "epoch": 4.476659901420701,
      "grad_norm": 6.925342082977295,
      "learning_rate": 4.617642176003281e-05,
      "loss": 13.0897,
      "step": 15440
    },
    {
      "epoch": 4.482458683676428,
      "grad_norm": 6.746035099029541,
      "learning_rate": 4.58615416021841e-05,
      "loss": 13.0345,
      "step": 15460
    },
    {
      "epoch": 4.488257465932154,
      "grad_norm": 6.476143836975098,
      "learning_rate": 4.553104323583804e-05,
      "loss": 13.0329,
      "step": 15480
    },
    {
      "epoch": 4.4940562481878805,
      "grad_norm": 6.641294956207275,
      "learning_rate": 4.520152682812051e-05,
      "loss": 13.0304,
      "step": 15500
    },
    {
      "epoch": 4.499855030443607,
      "grad_norm": 6.6077494621276855,
      "learning_rate": 4.487299547632925e-05,
      "loss": 13.017,
      "step": 15520
    },
    {
      "epoch": 4.505653812699333,
      "grad_norm": 7.050844192504883,
      "learning_rate": 4.4545452268503235e-05,
      "loss": 12.9833,
      "step": 15540
    },
    {
      "epoch": 4.51145259495506,
      "grad_norm": 6.745258331298828,
      "learning_rate": 4.421890028339306e-05,
      "loss": 12.9806,
      "step": 15560
    },
    {
      "epoch": 4.517251377210786,
      "grad_norm": 6.834493637084961,
      "learning_rate": 4.389334259043262e-05,
      "loss": 12.9973,
      "step": 15580
    },
    {
      "epoch": 4.5230501594665125,
      "grad_norm": 6.874848365783691,
      "learning_rate": 4.356878224970961e-05,
      "loss": 12.9055,
      "step": 15600
    },
    {
      "epoch": 4.528848941722238,
      "grad_norm": 7.373640537261963,
      "learning_rate": 4.3245222311937336e-05,
      "loss": 13.0708,
      "step": 15620
    },
    {
      "epoch": 4.534647723977964,
      "grad_norm": 6.888755798339844,
      "learning_rate": 4.292266581842576e-05,
      "loss": 12.9929,
      "step": 15640
    },
    {
      "epoch": 4.540446506233691,
      "grad_norm": 6.781831741333008,
      "learning_rate": 4.26011158010527e-05,
      "loss": 12.9452,
      "step": 15660
    },
    {
      "epoch": 4.546245288489417,
      "grad_norm": 7.088428497314453,
      "learning_rate": 4.228057528223599e-05,
      "loss": 12.9039,
      "step": 15680
    },
    {
      "epoch": 4.5520440707451435,
      "grad_norm": 7.105436325073242,
      "learning_rate": 4.1961047274904186e-05,
      "loss": 13.1016,
      "step": 15700
    },
    {
      "epoch": 4.55784285300087,
      "grad_norm": 6.834681510925293,
      "learning_rate": 4.164253478246915e-05,
      "loss": 12.8593,
      "step": 15720
    },
    {
      "epoch": 4.563641635256596,
      "grad_norm": 9.029895782470703,
      "learning_rate": 4.13250407987971e-05,
      "loss": 12.9066,
      "step": 15740
    },
    {
      "epoch": 4.569440417512323,
      "grad_norm": 7.267679214477539,
      "learning_rate": 4.100856830818088e-05,
      "loss": 12.8641,
      "step": 15760
    },
    {
      "epoch": 4.575239199768049,
      "grad_norm": 6.914674758911133,
      "learning_rate": 4.0693120285311725e-05,
      "loss": 12.9892,
      "step": 15780
    },
    {
      "epoch": 4.5810379820237745,
      "grad_norm": 6.6442036628723145,
      "learning_rate": 4.0378699695251424e-05,
      "loss": 12.9197,
      "step": 15800
    },
    {
      "epoch": 4.586836764279501,
      "grad_norm": 6.649363994598389,
      "learning_rate": 4.006530949340435e-05,
      "loss": 12.9803,
      "step": 15820
    },
    {
      "epoch": 4.592635546535227,
      "grad_norm": 6.563486576080322,
      "learning_rate": 3.975295262548962e-05,
      "loss": 12.8334,
      "step": 15840
    },
    {
      "epoch": 4.598434328790954,
      "grad_norm": 6.9590654373168945,
      "learning_rate": 3.9441632027513696e-05,
      "loss": 12.9293,
      "step": 15860
    },
    {
      "epoch": 4.60423311104668,
      "grad_norm": 6.973752975463867,
      "learning_rate": 3.9131350625742465e-05,
      "loss": 12.9695,
      "step": 15880
    },
    {
      "epoch": 4.6100318933024065,
      "grad_norm": 6.592644691467285,
      "learning_rate": 3.88221113366739e-05,
      "loss": 12.8906,
      "step": 15900
    },
    {
      "epoch": 4.615830675558133,
      "grad_norm": 6.745923042297363,
      "learning_rate": 3.851391706701057e-05,
      "loss": 12.855,
      "step": 15920
    },
    {
      "epoch": 4.621629457813859,
      "grad_norm": 7.179221153259277,
      "learning_rate": 3.8206770713632406e-05,
      "loss": 12.8531,
      "step": 15940
    },
    {
      "epoch": 4.627428240069586,
      "grad_norm": 6.872239589691162,
      "learning_rate": 3.790067516356942e-05,
      "loss": 12.8255,
      "step": 15960
    },
    {
      "epoch": 4.633227022325312,
      "grad_norm": 6.6904473304748535,
      "learning_rate": 3.7595633293974514e-05,
      "loss": 12.8377,
      "step": 15980
    },
    {
      "epoch": 4.639025804581038,
      "grad_norm": 6.703418254852295,
      "learning_rate": 3.729164797209659e-05,
      "loss": 12.8554,
      "step": 16000
    },
    {
      "epoch": 4.644824586836764,
      "grad_norm": 7.0732197761535645,
      "learning_rate": 3.6988722055253396e-05,
      "loss": 12.8836,
      "step": 16020
    },
    {
      "epoch": 4.65062336909249,
      "grad_norm": 6.85878849029541,
      "learning_rate": 3.668685839080486e-05,
      "loss": 12.812,
      "step": 16040
    },
    {
      "epoch": 4.656422151348217,
      "grad_norm": 6.760440826416016,
      "learning_rate": 3.6386059816126194e-05,
      "loss": 12.9691,
      "step": 16060
    },
    {
      "epoch": 4.662220933603943,
      "grad_norm": 6.90209436416626,
      "learning_rate": 3.60863291585813e-05,
      "loss": 12.794,
      "step": 16080
    },
    {
      "epoch": 4.6680197158596695,
      "grad_norm": 7.593218803405762,
      "learning_rate": 3.57876692354961e-05,
      "loss": 12.8864,
      "step": 16100
    },
    {
      "epoch": 4.673818498115396,
      "grad_norm": 6.5068182945251465,
      "learning_rate": 3.549008285413231e-05,
      "loss": 12.8848,
      "step": 16120
    },
    {
      "epoch": 4.679617280371122,
      "grad_norm": 6.809333324432373,
      "learning_rate": 3.519357281166059e-05,
      "loss": 12.7047,
      "step": 16140
    },
    {
      "epoch": 4.685416062626849,
      "grad_norm": 7.446491241455078,
      "learning_rate": 3.48981418951347e-05,
      "loss": 12.8969,
      "step": 16160
    },
    {
      "epoch": 4.691214844882575,
      "grad_norm": 6.474913597106934,
      "learning_rate": 3.460379288146509e-05,
      "loss": 12.7725,
      "step": 16180
    },
    {
      "epoch": 4.6970136271383005,
      "grad_norm": 6.5087056159973145,
      "learning_rate": 3.431052853739278e-05,
      "loss": 12.7652,
      "step": 16200
    },
    {
      "epoch": 4.702812409394027,
      "grad_norm": 7.135810852050781,
      "learning_rate": 3.401835161946363e-05,
      "loss": 12.8104,
      "step": 16220
    },
    {
      "epoch": 4.708611191649753,
      "grad_norm": 6.978041172027588,
      "learning_rate": 3.372726487400184e-05,
      "loss": 12.7355,
      "step": 16240
    },
    {
      "epoch": 4.71440997390548,
      "grad_norm": 6.751758575439453,
      "learning_rate": 3.3437271037084884e-05,
      "loss": 12.8518,
      "step": 16260
    },
    {
      "epoch": 4.720208756161206,
      "grad_norm": 6.598989486694336,
      "learning_rate": 3.3148372834517106e-05,
      "loss": 12.8532,
      "step": 16280
    },
    {
      "epoch": 4.726007538416932,
      "grad_norm": 6.994054317474365,
      "learning_rate": 3.2860572981804676e-05,
      "loss": 12.7759,
      "step": 16300
    },
    {
      "epoch": 4.731806320672659,
      "grad_norm": 6.816269874572754,
      "learning_rate": 3.2573874184129546e-05,
      "loss": 12.8446,
      "step": 16320
    },
    {
      "epoch": 4.737605102928385,
      "grad_norm": 6.80454683303833,
      "learning_rate": 3.22882791363244e-05,
      "loss": 12.9482,
      "step": 16340
    },
    {
      "epoch": 4.743403885184112,
      "grad_norm": 6.6721038818359375,
      "learning_rate": 3.200379052284731e-05,
      "loss": 12.8748,
      "step": 16360
    },
    {
      "epoch": 4.749202667439838,
      "grad_norm": 7.064223289489746,
      "learning_rate": 3.172041101775608e-05,
      "loss": 12.8468,
      "step": 16380
    },
    {
      "epoch": 4.755001449695564,
      "grad_norm": 6.886677265167236,
      "learning_rate": 3.143814328468382e-05,
      "loss": 12.8152,
      "step": 16400
    },
    {
      "epoch": 4.76080023195129,
      "grad_norm": 6.575488567352295,
      "learning_rate": 3.115698997681308e-05,
      "loss": 12.7584,
      "step": 16420
    },
    {
      "epoch": 4.766599014207016,
      "grad_norm": 6.668886184692383,
      "learning_rate": 3.087695373685173e-05,
      "loss": 12.7695,
      "step": 16440
    },
    {
      "epoch": 4.772397796462743,
      "grad_norm": 7.102806568145752,
      "learning_rate": 3.0598037197007485e-05,
      "loss": 12.7779,
      "step": 16460
    },
    {
      "epoch": 4.778196578718469,
      "grad_norm": 6.7956223487854,
      "learning_rate": 3.0320242978963523e-05,
      "loss": 12.7672,
      "step": 16480
    },
    {
      "epoch": 4.783995360974195,
      "grad_norm": 6.959965705871582,
      "learning_rate": 3.0043573693853664e-05,
      "loss": 12.7644,
      "step": 16500
    },
    {
      "epoch": 4.789794143229922,
      "grad_norm": 6.672738075256348,
      "learning_rate": 2.976803194223794e-05,
      "loss": 12.732,
      "step": 16520
    },
    {
      "epoch": 4.795592925485648,
      "grad_norm": 6.367424488067627,
      "learning_rate": 2.949362031407812e-05,
      "loss": 12.7009,
      "step": 16540
    },
    {
      "epoch": 4.801391707741375,
      "grad_norm": 6.8179168701171875,
      "learning_rate": 2.922034138871321e-05,
      "loss": 12.6847,
      "step": 16560
    },
    {
      "epoch": 4.807190489997101,
      "grad_norm": 6.528498649597168,
      "learning_rate": 2.894819773483555e-05,
      "loss": 12.7785,
      "step": 16580
    },
    {
      "epoch": 4.8129892722528265,
      "grad_norm": 6.769229412078857,
      "learning_rate": 2.867719191046632e-05,
      "loss": 12.6907,
      "step": 16600
    },
    {
      "epoch": 4.818788054508553,
      "grad_norm": 6.2879557609558105,
      "learning_rate": 2.8407326462931735e-05,
      "loss": 12.66,
      "step": 16620
    },
    {
      "epoch": 4.824586836764279,
      "grad_norm": 6.167392730712891,
      "learning_rate": 2.8138603928838922e-05,
      "loss": 12.7354,
      "step": 16640
    },
    {
      "epoch": 4.830385619020006,
      "grad_norm": 6.677105903625488,
      "learning_rate": 2.7871026834052214e-05,
      "loss": 12.669,
      "step": 16660
    },
    {
      "epoch": 4.836184401275732,
      "grad_norm": 6.4421772956848145,
      "learning_rate": 2.7604597693669374e-05,
      "loss": 12.7254,
      "step": 16680
    },
    {
      "epoch": 4.841983183531458,
      "grad_norm": 6.49321985244751,
      "learning_rate": 2.733931901199788e-05,
      "loss": 12.6479,
      "step": 16700
    },
    {
      "epoch": 4.847781965787185,
      "grad_norm": 7.041791915893555,
      "learning_rate": 2.707519328253151e-05,
      "loss": 12.7824,
      "step": 16720
    },
    {
      "epoch": 4.853580748042911,
      "grad_norm": 6.61463737487793,
      "learning_rate": 2.6812222987926796e-05,
      "loss": 12.7032,
      "step": 16740
    },
    {
      "epoch": 4.859379530298638,
      "grad_norm": 6.492900371551514,
      "learning_rate": 2.655041059997972e-05,
      "loss": 12.6592,
      "step": 16760
    },
    {
      "epoch": 4.865178312554364,
      "grad_norm": 6.857891082763672,
      "learning_rate": 2.6289758579602544e-05,
      "loss": 12.6406,
      "step": 16780
    },
    {
      "epoch": 4.87097709481009,
      "grad_norm": 6.472780704498291,
      "learning_rate": 2.6030269376800593e-05,
      "loss": 12.7107,
      "step": 16800
    },
    {
      "epoch": 4.876775877065816,
      "grad_norm": 6.676541328430176,
      "learning_rate": 2.57719454306492e-05,
      "loss": 12.7329,
      "step": 16820
    },
    {
      "epoch": 4.882574659321542,
      "grad_norm": 6.0651373863220215,
      "learning_rate": 2.551478916927101e-05,
      "loss": 12.6973,
      "step": 16840
    },
    {
      "epoch": 4.888373441577269,
      "grad_norm": 6.476709365844727,
      "learning_rate": 2.525880300981278e-05,
      "loss": 12.7434,
      "step": 16860
    },
    {
      "epoch": 4.894172223832995,
      "grad_norm": 6.469247817993164,
      "learning_rate": 2.500398935842298e-05,
      "loss": 12.591,
      "step": 16880
    },
    {
      "epoch": 4.899971006088721,
      "grad_norm": 6.322939395904541,
      "learning_rate": 2.4750350610229036e-05,
      "loss": 12.6011,
      "step": 16900
    },
    {
      "epoch": 4.905769788344448,
      "grad_norm": 6.4993414878845215,
      "learning_rate": 2.449788914931482e-05,
      "loss": 12.6326,
      "step": 16920
    },
    {
      "epoch": 4.911568570600174,
      "grad_norm": 6.611610412597656,
      "learning_rate": 2.4246607348698384e-05,
      "loss": 12.6734,
      "step": 16940
    },
    {
      "epoch": 4.9173673528559005,
      "grad_norm": 6.7235918045043945,
      "learning_rate": 2.399650757030929e-05,
      "loss": 12.6932,
      "step": 16960
    },
    {
      "epoch": 4.923166135111627,
      "grad_norm": 6.817465782165527,
      "learning_rate": 2.374759216496694e-05,
      "loss": 12.7284,
      "step": 16980
    },
    {
      "epoch": 4.928964917367352,
      "grad_norm": 6.2077741622924805,
      "learning_rate": 2.349986347235788e-05,
      "loss": 12.6141,
      "step": 17000
    },
    {
      "epoch": 4.934763699623079,
      "grad_norm": 6.823101043701172,
      "learning_rate": 2.3253323821014473e-05,
      "loss": 12.6669,
      "step": 17020
    },
    {
      "epoch": 4.940562481878805,
      "grad_norm": 6.101616859436035,
      "learning_rate": 2.3007975528292337e-05,
      "loss": 12.6686,
      "step": 17040
    },
    {
      "epoch": 4.946361264134532,
      "grad_norm": 6.41456937789917,
      "learning_rate": 2.276382090034902e-05,
      "loss": 12.71,
      "step": 17060
    },
    {
      "epoch": 4.952160046390258,
      "grad_norm": 6.453919410705566,
      "learning_rate": 2.2520862232122318e-05,
      "loss": 12.6648,
      "step": 17080
    },
    {
      "epoch": 4.957958828645984,
      "grad_norm": 6.823217868804932,
      "learning_rate": 2.227910180730825e-05,
      "loss": 12.544,
      "step": 17100
    },
    {
      "epoch": 4.963757610901711,
      "grad_norm": 6.6497883796691895,
      "learning_rate": 2.203854189834029e-05,
      "loss": 12.623,
      "step": 17120
    },
    {
      "epoch": 4.969556393157437,
      "grad_norm": 6.425251007080078,
      "learning_rate": 2.1799184766367278e-05,
      "loss": 12.6152,
      "step": 17140
    },
    {
      "epoch": 4.9753551754131635,
      "grad_norm": 6.576930999755859,
      "learning_rate": 2.156103266123283e-05,
      "loss": 12.4449,
      "step": 17160
    },
    {
      "epoch": 4.98115395766889,
      "grad_norm": 6.7270331382751465,
      "learning_rate": 2.1324087821453683e-05,
      "loss": 12.6827,
      "step": 17180
    },
    {
      "epoch": 4.986952739924616,
      "grad_norm": 6.390686511993408,
      "learning_rate": 2.1088352474198922e-05,
      "loss": 12.5817,
      "step": 17200
    },
    {
      "epoch": 4.992751522180342,
      "grad_norm": 6.215938568115234,
      "learning_rate": 2.085382883526897e-05,
      "loss": 12.5647,
      "step": 17220
    },
    {
      "epoch": 4.998550304436068,
      "grad_norm": 6.623614311218262,
      "learning_rate": 2.0620519109074752e-05,
      "loss": 12.6924,
      "step": 17240
    },
    {
      "epoch": 5.0,
      "eval_loss": 13.045024871826172,
      "eval_runtime": 178.9016,
      "eval_samples_per_second": 45.791,
      "eval_steps_per_second": 5.724,
      "step": 17245
    },
    {
      "epoch": 5.004349086691795,
      "grad_norm": 5.796150207519531,
      "learning_rate": 2.0388425488617045e-05,
      "loss": 12.5846,
      "step": 17260
    },
    {
      "epoch": 5.010147868947521,
      "grad_norm": 6.239770412445068,
      "learning_rate": 2.015755015546566e-05,
      "loss": 12.4999,
      "step": 17280
    },
    {
      "epoch": 5.015946651203247,
      "grad_norm": 6.616846561431885,
      "learning_rate": 1.9927895279739274e-05,
      "loss": 12.4648,
      "step": 17300
    },
    {
      "epoch": 5.021745433458974,
      "grad_norm": 6.007425308227539,
      "learning_rate": 1.969946302008476e-05,
      "loss": 12.5785,
      "step": 17320
    },
    {
      "epoch": 5.0275442157147,
      "grad_norm": 6.332566738128662,
      "learning_rate": 1.9472255523656964e-05,
      "loss": 12.471,
      "step": 17340
    },
    {
      "epoch": 5.0333429979704265,
      "grad_norm": 6.252227783203125,
      "learning_rate": 1.9246274926098555e-05,
      "loss": 12.5093,
      "step": 17360
    },
    {
      "epoch": 5.039141780226153,
      "grad_norm": 6.233333110809326,
      "learning_rate": 1.902152335151993e-05,
      "loss": 12.4636,
      "step": 17380
    },
    {
      "epoch": 5.044940562481878,
      "grad_norm": 6.1287455558776855,
      "learning_rate": 1.8798002912479254e-05,
      "loss": 12.4553,
      "step": 17400
    },
    {
      "epoch": 5.050739344737605,
      "grad_norm": 6.130240440368652,
      "learning_rate": 1.8575715709962613e-05,
      "loss": 12.5477,
      "step": 17420
    },
    {
      "epoch": 5.056538126993331,
      "grad_norm": 6.092936038970947,
      "learning_rate": 1.8354663833364232e-05,
      "loss": 12.5597,
      "step": 17440
    },
    {
      "epoch": 5.0623369092490575,
      "grad_norm": 6.333241939544678,
      "learning_rate": 1.81348493604669e-05,
      "loss": 12.6003,
      "step": 17460
    },
    {
      "epoch": 5.068135691504784,
      "grad_norm": 6.362922668457031,
      "learning_rate": 1.7916274357422346e-05,
      "loss": 12.3886,
      "step": 17480
    },
    {
      "epoch": 5.07393447376051,
      "grad_norm": 6.252371311187744,
      "learning_rate": 1.7698940878731926e-05,
      "loss": 12.5373,
      "step": 17500
    },
    {
      "epoch": 5.079733256016237,
      "grad_norm": 6.347475528717041,
      "learning_rate": 1.7482850967227185e-05,
      "loss": 12.4083,
      "step": 17520
    },
    {
      "epoch": 5.085532038271963,
      "grad_norm": 6.197582721710205,
      "learning_rate": 1.726800665405076e-05,
      "loss": 12.3475,
      "step": 17540
    },
    {
      "epoch": 5.0913308205276895,
      "grad_norm": 6.359958171844482,
      "learning_rate": 1.7054409958637367e-05,
      "loss": 12.3785,
      "step": 17560
    },
    {
      "epoch": 5.097129602783416,
      "grad_norm": 6.169374465942383,
      "learning_rate": 1.6842062888694492e-05,
      "loss": 12.4445,
      "step": 17580
    },
    {
      "epoch": 5.102928385039141,
      "grad_norm": 6.084906101226807,
      "learning_rate": 1.6630967440183863e-05,
      "loss": 12.3395,
      "step": 17600
    },
    {
      "epoch": 5.108727167294868,
      "grad_norm": 5.979246616363525,
      "learning_rate": 1.6421125597302572e-05,
      "loss": 12.4911,
      "step": 17620
    },
    {
      "epoch": 5.114525949550594,
      "grad_norm": 6.302183151245117,
      "learning_rate": 1.6212539332464346e-05,
      "loss": 12.4454,
      "step": 17640
    },
    {
      "epoch": 5.1203247318063205,
      "grad_norm": 6.460176944732666,
      "learning_rate": 1.6005210606281184e-05,
      "loss": 12.4227,
      "step": 17660
    },
    {
      "epoch": 5.126123514062047,
      "grad_norm": 5.949193477630615,
      "learning_rate": 1.5799141367544636e-05,
      "loss": 12.4855,
      "step": 17680
    },
    {
      "epoch": 5.131922296317773,
      "grad_norm": 6.036619663238525,
      "learning_rate": 1.5594333553207895e-05,
      "loss": 12.4832,
      "step": 17700
    },
    {
      "epoch": 5.1377210785735,
      "grad_norm": 6.535499572753906,
      "learning_rate": 1.5390789088367134e-05,
      "loss": 12.4968,
      "step": 17720
    },
    {
      "epoch": 5.143519860829226,
      "grad_norm": 5.857772350311279,
      "learning_rate": 1.5188509886243871e-05,
      "loss": 12.4628,
      "step": 17740
    },
    {
      "epoch": 5.1493186430849525,
      "grad_norm": 6.254627704620361,
      "learning_rate": 1.4987497848166518e-05,
      "loss": 12.3921,
      "step": 17760
    },
    {
      "epoch": 5.155117425340679,
      "grad_norm": 6.29732608795166,
      "learning_rate": 1.4787754863552897e-05,
      "loss": 12.4548,
      "step": 17780
    },
    {
      "epoch": 5.160916207596404,
      "grad_norm": 6.085211753845215,
      "learning_rate": 1.4589282809892372e-05,
      "loss": 12.4842,
      "step": 17800
    },
    {
      "epoch": 5.166714989852131,
      "grad_norm": 6.15186071395874,
      "learning_rate": 1.4392083552727962e-05,
      "loss": 12.4414,
      "step": 17820
    },
    {
      "epoch": 5.172513772107857,
      "grad_norm": 5.959420204162598,
      "learning_rate": 1.4196158945639224e-05,
      "loss": 12.4399,
      "step": 17840
    },
    {
      "epoch": 5.1783125543635835,
      "grad_norm": 6.11055326461792,
      "learning_rate": 1.4001510830224433e-05,
      "loss": 12.3928,
      "step": 17860
    },
    {
      "epoch": 5.18411133661931,
      "grad_norm": 6.096806526184082,
      "learning_rate": 1.3808141036083576e-05,
      "loss": 12.4109,
      "step": 17880
    },
    {
      "epoch": 5.189910118875036,
      "grad_norm": 6.0901618003845215,
      "learning_rate": 1.3616051380800952e-05,
      "loss": 12.3867,
      "step": 17900
    },
    {
      "epoch": 5.195708901130763,
      "grad_norm": 6.269711017608643,
      "learning_rate": 1.3425243669928205e-05,
      "loss": 12.3882,
      "step": 17920
    },
    {
      "epoch": 5.201507683386489,
      "grad_norm": 6.299615859985352,
      "learning_rate": 1.3235719696967262e-05,
      "loss": 12.293,
      "step": 17940
    },
    {
      "epoch": 5.207306465642215,
      "grad_norm": 6.018105506896973,
      "learning_rate": 1.3047481243353558e-05,
      "loss": 12.4394,
      "step": 17960
    },
    {
      "epoch": 5.213105247897942,
      "grad_norm": 6.464113235473633,
      "learning_rate": 1.2860530078439275e-05,
      "loss": 12.4359,
      "step": 17980
    },
    {
      "epoch": 5.218904030153667,
      "grad_norm": 6.218840599060059,
      "learning_rate": 1.2674867959476587e-05,
      "loss": 12.418,
      "step": 18000
    },
    {
      "epoch": 5.224702812409394,
      "grad_norm": 6.096217155456543,
      "learning_rate": 1.2490496631601399e-05,
      "loss": 12.3445,
      "step": 18020
    },
    {
      "epoch": 5.23050159466512,
      "grad_norm": 6.106896877288818,
      "learning_rate": 1.2307417827816702e-05,
      "loss": 12.451,
      "step": 18040
    },
    {
      "epoch": 5.2363003769208465,
      "grad_norm": 6.021442413330078,
      "learning_rate": 1.2125633268976386e-05,
      "loss": 12.3363,
      "step": 18060
    },
    {
      "epoch": 5.242099159176573,
      "grad_norm": 5.9259185791015625,
      "learning_rate": 1.1945144663769064e-05,
      "loss": 12.4185,
      "step": 18080
    },
    {
      "epoch": 5.247897941432299,
      "grad_norm": 6.4642767906188965,
      "learning_rate": 1.1765953708701964e-05,
      "loss": 12.3899,
      "step": 18100
    },
    {
      "epoch": 5.253696723688026,
      "grad_norm": 6.202354907989502,
      "learning_rate": 1.1588062088085082e-05,
      "loss": 12.5179,
      "step": 18120
    },
    {
      "epoch": 5.259495505943752,
      "grad_norm": 5.941465854644775,
      "learning_rate": 1.1411471474015228e-05,
      "loss": 12.3256,
      "step": 18140
    },
    {
      "epoch": 5.265294288199478,
      "grad_norm": 6.068994998931885,
      "learning_rate": 1.1236183526360393e-05,
      "loss": 12.3883,
      "step": 18160
    },
    {
      "epoch": 5.271093070455205,
      "grad_norm": 6.093503475189209,
      "learning_rate": 1.1062199892744133e-05,
      "loss": 12.3282,
      "step": 18180
    },
    {
      "epoch": 5.27689185271093,
      "grad_norm": 6.309504508972168,
      "learning_rate": 1.0889522208530043e-05,
      "loss": 12.3889,
      "step": 18200
    },
    {
      "epoch": 5.282690634966657,
      "grad_norm": 5.914751052856445,
      "learning_rate": 1.0718152096806437e-05,
      "loss": 12.3671,
      "step": 18220
    },
    {
      "epoch": 5.288489417222383,
      "grad_norm": 6.496561527252197,
      "learning_rate": 1.054809116837106e-05,
      "loss": 12.3548,
      "step": 18240
    },
    {
      "epoch": 5.2942881994781095,
      "grad_norm": 6.467747211456299,
      "learning_rate": 1.037934102171593e-05,
      "loss": 12.4741,
      "step": 18260
    },
    {
      "epoch": 5.300086981733836,
      "grad_norm": 6.421821117401123,
      "learning_rate": 1.0211903243012415e-05,
      "loss": 12.3358,
      "step": 18280
    },
    {
      "epoch": 5.305885763989562,
      "grad_norm": 6.510556221008301,
      "learning_rate": 1.0045779406096156e-05,
      "loss": 12.3716,
      "step": 18300
    },
    {
      "epoch": 5.311684546245289,
      "grad_norm": 6.181366443634033,
      "learning_rate": 9.88097107245241e-06,
      "loss": 12.2273,
      "step": 18320
    },
    {
      "epoch": 5.317483328501015,
      "grad_norm": 6.0905842781066895,
      "learning_rate": 9.71747979120131e-06,
      "loss": 12.4248,
      "step": 18340
    },
    {
      "epoch": 5.323282110756741,
      "grad_norm": 6.4256744384765625,
      "learning_rate": 9.555307099083326e-06,
      "loss": 12.4015,
      "step": 18360
    },
    {
      "epoch": 5.329080893012468,
      "grad_norm": 6.045983791351318,
      "learning_rate": 9.394454520444872e-06,
      "loss": 12.3454,
      "step": 18380
    },
    {
      "epoch": 5.334879675268193,
      "grad_norm": 6.347003936767578,
      "learning_rate": 9.234923567223807e-06,
      "loss": 12.3579,
      "step": 18400
    },
    {
      "epoch": 5.34067845752392,
      "grad_norm": 6.118569850921631,
      "learning_rate": 9.076715738935458e-06,
      "loss": 12.4214,
      "step": 18420
    },
    {
      "epoch": 5.346477239779646,
      "grad_norm": 6.039132118225098,
      "learning_rate": 8.919832522658288e-06,
      "loss": 12.2925,
      "step": 18440
    },
    {
      "epoch": 5.3522760220353724,
      "grad_norm": 6.005889415740967,
      "learning_rate": 8.764275393020165e-06,
      "loss": 12.3006,
      "step": 18460
    },
    {
      "epoch": 5.358074804291099,
      "grad_norm": 6.1089582443237305,
      "learning_rate": 8.610045812184264e-06,
      "loss": 12.3102,
      "step": 18480
    },
    {
      "epoch": 5.363873586546825,
      "grad_norm": 6.284377574920654,
      "learning_rate": 8.457145229835438e-06,
      "loss": 12.411,
      "step": 18500
    },
    {
      "epoch": 5.369672368802552,
      "grad_norm": 5.929565906524658,
      "learning_rate": 8.305575083166705e-06,
      "loss": 12.4191,
      "step": 18520
    },
    {
      "epoch": 5.375471151058278,
      "grad_norm": 6.123082637786865,
      "learning_rate": 8.155336796865447e-06,
      "loss": 12.3731,
      "step": 18540
    },
    {
      "epoch": 5.381269933314004,
      "grad_norm": 6.017785549163818,
      "learning_rate": 8.006431783100347e-06,
      "loss": 12.393,
      "step": 18560
    },
    {
      "epoch": 5.387068715569731,
      "grad_norm": 5.97633695602417,
      "learning_rate": 7.858861441507796e-06,
      "loss": 12.3182,
      "step": 18580
    },
    {
      "epoch": 5.392867497825456,
      "grad_norm": 5.912225723266602,
      "learning_rate": 7.712627159179085e-06,
      "loss": 12.3775,
      "step": 18600
    },
    {
      "epoch": 5.398666280081183,
      "grad_norm": 6.072891712188721,
      "learning_rate": 7.567730310647035e-06,
      "loss": 12.4808,
      "step": 18620
    },
    {
      "epoch": 5.404465062336909,
      "grad_norm": 5.797310829162598,
      "learning_rate": 7.42417225787329e-06,
      "loss": 12.3386,
      "step": 18640
    },
    {
      "epoch": 5.410263844592635,
      "grad_norm": 6.016908645629883,
      "learning_rate": 7.281954350235442e-06,
      "loss": 12.3342,
      "step": 18660
    },
    {
      "epoch": 5.416062626848362,
      "grad_norm": 6.35310697555542,
      "learning_rate": 7.141077924514327e-06,
      "loss": 12.3712,
      "step": 18680
    },
    {
      "epoch": 5.421861409104088,
      "grad_norm": 5.977181434631348,
      "learning_rate": 7.001544304881551e-06,
      "loss": 12.3994,
      "step": 18700
    },
    {
      "epoch": 5.427660191359815,
      "grad_norm": 6.214212417602539,
      "learning_rate": 6.863354802886867e-06,
      "loss": 12.3855,
      "step": 18720
    },
    {
      "epoch": 5.433458973615541,
      "grad_norm": 5.722358703613281,
      "learning_rate": 6.726510717446082e-06,
      "loss": 12.3312,
      "step": 18740
    },
    {
      "epoch": 5.439257755871267,
      "grad_norm": 6.0751051902771,
      "learning_rate": 6.591013334828654e-06,
      "loss": 12.3936,
      "step": 18760
    },
    {
      "epoch": 5.445056538126993,
      "grad_norm": 6.28613805770874,
      "learning_rate": 6.4568639286457e-06,
      "loss": 12.3232,
      "step": 18780
    },
    {
      "epoch": 5.450855320382719,
      "grad_norm": 6.0690836906433105,
      "learning_rate": 6.324063759838005e-06,
      "loss": 12.3313,
      "step": 18800
    },
    {
      "epoch": 5.456654102638446,
      "grad_norm": 5.831625938415527,
      "learning_rate": 6.192614076664132e-06,
      "loss": 12.4137,
      "step": 18820
    },
    {
      "epoch": 5.462452884894172,
      "grad_norm": 6.215555191040039,
      "learning_rate": 6.062516114688732e-06,
      "loss": 12.3621,
      "step": 18840
    },
    {
      "epoch": 5.468251667149898,
      "grad_norm": 5.936301231384277,
      "learning_rate": 5.933771096770934e-06,
      "loss": 12.3736,
      "step": 18860
    },
    {
      "epoch": 5.474050449405625,
      "grad_norm": 6.107048988342285,
      "learning_rate": 5.80638023305281e-06,
      "loss": 12.4379,
      "step": 18880
    },
    {
      "epoch": 5.479849231661351,
      "grad_norm": 6.0982842445373535,
      "learning_rate": 5.68034472094806e-06,
      "loss": 12.3416,
      "step": 18900
    },
    {
      "epoch": 5.485648013917078,
      "grad_norm": 5.772578239440918,
      "learning_rate": 5.555665745130694e-06,
      "loss": 12.3391,
      "step": 18920
    },
    {
      "epoch": 5.491446796172804,
      "grad_norm": 6.0487961769104,
      "learning_rate": 5.43234447752392e-06,
      "loss": 12.4237,
      "step": 18940
    },
    {
      "epoch": 5.49724557842853,
      "grad_norm": 5.959697723388672,
      "learning_rate": 5.310382077289138e-06,
      "loss": 12.3024,
      "step": 18960
    },
    {
      "epoch": 5.503044360684257,
      "grad_norm": 5.968181610107422,
      "learning_rate": 5.1897796908150656e-06,
      "loss": 12.2864,
      "step": 18980
    },
    {
      "epoch": 5.508843142939982,
      "grad_norm": 5.740213871002197,
      "learning_rate": 5.070538451706912e-06,
      "loss": 12.2923,
      "step": 19000
    },
    {
      "epoch": 5.514641925195709,
      "grad_norm": 6.296136379241943,
      "learning_rate": 4.9526594807757395e-06,
      "loss": 12.3298,
      "step": 19020
    },
    {
      "epoch": 5.520440707451435,
      "grad_norm": 6.081227779388428,
      "learning_rate": 4.836143886027965e-06,
      "loss": 12.3523,
      "step": 19040
    },
    {
      "epoch": 5.526239489707161,
      "grad_norm": 6.25449800491333,
      "learning_rate": 4.720992762654879e-06,
      "loss": 12.3889,
      "step": 19060
    },
    {
      "epoch": 5.532038271962888,
      "grad_norm": 5.974481582641602,
      "learning_rate": 4.6072071930223946e-06,
      "loss": 12.3611,
      "step": 19080
    },
    {
      "epoch": 5.537837054218614,
      "grad_norm": 6.336782932281494,
      "learning_rate": 4.494788246660941e-06,
      "loss": 12.4171,
      "step": 19100
    },
    {
      "epoch": 5.5436358364743406,
      "grad_norm": 6.1892313957214355,
      "learning_rate": 4.383736980255209e-06,
      "loss": 12.3181,
      "step": 19120
    },
    {
      "epoch": 5.549434618730067,
      "grad_norm": 6.390849590301514,
      "learning_rate": 4.274054437634489e-06,
      "loss": 12.39,
      "step": 19140
    },
    {
      "epoch": 5.555233400985793,
      "grad_norm": 5.988072395324707,
      "learning_rate": 4.165741649762544e-06,
      "loss": 12.3414,
      "step": 19160
    },
    {
      "epoch": 5.561032183241519,
      "grad_norm": 6.2669878005981445,
      "learning_rate": 4.058799634728288e-06,
      "loss": 12.3108,
      "step": 19180
    },
    {
      "epoch": 5.566830965497245,
      "grad_norm": 5.839949131011963,
      "learning_rate": 3.953229397735875e-06,
      "loss": 12.2824,
      "step": 19200
    },
    {
      "epoch": 5.572629747752972,
      "grad_norm": 6.266679286956787,
      "learning_rate": 3.84903193109542e-06,
      "loss": 12.2983,
      "step": 19220
    },
    {
      "epoch": 5.578428530008698,
      "grad_norm": 6.4383978843688965,
      "learning_rate": 3.7462082142137472e-06,
      "loss": 12.2928,
      "step": 19240
    },
    {
      "epoch": 5.584227312264424,
      "grad_norm": 6.34873628616333,
      "learning_rate": 3.6447592135849245e-06,
      "loss": 12.2558,
      "step": 19260
    },
    {
      "epoch": 5.590026094520151,
      "grad_norm": 6.178395748138428,
      "learning_rate": 3.544685882781523e-06,
      "loss": 12.3794,
      "step": 19280
    },
    {
      "epoch": 5.595824876775877,
      "grad_norm": 6.0116729736328125,
      "learning_rate": 3.445989162445273e-06,
      "loss": 12.2978,
      "step": 19300
    },
    {
      "epoch": 5.6016236590316035,
      "grad_norm": 5.674201011657715,
      "learning_rate": 3.3486699802785732e-06,
      "loss": 12.2724,
      "step": 19320
    },
    {
      "epoch": 5.60742244128733,
      "grad_norm": 6.06737756729126,
      "learning_rate": 3.252729251035563e-06,
      "loss": 12.3675,
      "step": 19340
    },
    {
      "epoch": 5.613221223543056,
      "grad_norm": 6.566035747528076,
      "learning_rate": 3.1581678765135786e-06,
      "loss": 12.3449,
      "step": 19360
    },
    {
      "epoch": 5.619020005798783,
      "grad_norm": 6.122481822967529,
      "learning_rate": 3.064986745544662e-06,
      "loss": 12.1966,
      "step": 19380
    },
    {
      "epoch": 5.624818788054508,
      "grad_norm": 6.1720075607299805,
      "learning_rate": 2.9731867339872315e-06,
      "loss": 12.3642,
      "step": 19400
    },
    {
      "epoch": 5.630617570310235,
      "grad_norm": 5.763163089752197,
      "learning_rate": 2.882768704717875e-06,
      "loss": 12.2759,
      "step": 19420
    },
    {
      "epoch": 5.636416352565961,
      "grad_norm": 6.041781902313232,
      "learning_rate": 2.7937335076231026e-06,
      "loss": 12.2875,
      "step": 19440
    },
    {
      "epoch": 5.642215134821687,
      "grad_norm": 6.157629489898682,
      "learning_rate": 2.70608197959159e-06,
      "loss": 12.4376,
      "step": 19460
    },
    {
      "epoch": 5.648013917077414,
      "grad_norm": 5.921782970428467,
      "learning_rate": 2.619814944506099e-06,
      "loss": 12.2799,
      "step": 19480
    },
    {
      "epoch": 5.65381269933314,
      "grad_norm": 5.978129863739014,
      "learning_rate": 2.5349332132358014e-06,
      "loss": 12.3404,
      "step": 19500
    },
    {
      "epoch": 5.6596114815888665,
      "grad_norm": 5.922969341278076,
      "learning_rate": 2.455579433026139e-06,
      "loss": 12.2303,
      "step": 19520
    },
    {
      "epoch": 5.665410263844593,
      "grad_norm": 6.0823235511779785,
      "learning_rate": 2.373401327139196e-06,
      "loss": 12.3586,
      "step": 19540
    },
    {
      "epoch": 5.671209046100319,
      "grad_norm": 6.033995628356934,
      "learning_rate": 2.2926108412386013e-06,
      "loss": 12.3296,
      "step": 19560
    },
    {
      "epoch": 5.677007828356045,
      "grad_norm": 5.918379306793213,
      "learning_rate": 2.213208734716504e-06,
      "loss": 12.2942,
      "step": 19580
    },
    {
      "epoch": 5.682806610611771,
      "grad_norm": 6.067556381225586,
      "learning_rate": 2.1351957539151086e-06,
      "loss": 12.3557,
      "step": 19600
    },
    {
      "epoch": 5.688605392867498,
      "grad_norm": 5.866916656494141,
      "learning_rate": 2.058572632119382e-06,
      "loss": 12.2351,
      "step": 19620
    },
    {
      "epoch": 5.694404175123224,
      "grad_norm": 6.508310794830322,
      "learning_rate": 1.983340089550339e-06,
      "loss": 12.428,
      "step": 19640
    },
    {
      "epoch": 5.70020295737895,
      "grad_norm": 6.477244853973389,
      "learning_rate": 1.909498833358153e-06,
      "loss": 12.2988,
      "step": 19660
    },
    {
      "epoch": 5.706001739634677,
      "grad_norm": 6.11742639541626,
      "learning_rate": 1.8370495576156386e-06,
      "loss": 12.3967,
      "step": 19680
    },
    {
      "epoch": 5.711800521890403,
      "grad_norm": 5.718700408935547,
      "learning_rate": 1.7659929433115784e-06,
      "loss": 12.2919,
      "step": 19700
    },
    {
      "epoch": 5.7175993041461295,
      "grad_norm": 6.513905048370361,
      "learning_rate": 1.696329658344442e-06,
      "loss": 12.3291,
      "step": 19720
    },
    {
      "epoch": 5.723398086401856,
      "grad_norm": 6.16771125793457,
      "learning_rate": 1.6280603575160756e-06,
      "loss": 12.3179,
      "step": 19740
    },
    {
      "epoch": 5.729196868657582,
      "grad_norm": 6.029636383056641,
      "learning_rate": 1.5611856825255064e-06,
      "loss": 12.2774,
      "step": 19760
    },
    {
      "epoch": 5.734995650913309,
      "grad_norm": 5.909746170043945,
      "learning_rate": 1.4957062619629313e-06,
      "loss": 12.1921,
      "step": 19780
    },
    {
      "epoch": 5.740794433169034,
      "grad_norm": 6.003148078918457,
      "learning_rate": 1.4316227113038702e-06,
      "loss": 12.2661,
      "step": 19800
    },
    {
      "epoch": 5.7465932154247605,
      "grad_norm": 6.129434108734131,
      "learning_rate": 1.368935632903273e-06,
      "loss": 12.2361,
      "step": 19820
    },
    {
      "epoch": 5.752391997680487,
      "grad_norm": 6.235040664672852,
      "learning_rate": 1.3076456159899384e-06,
      "loss": 12.3324,
      "step": 19840
    },
    {
      "epoch": 5.758190779936213,
      "grad_norm": 5.995519161224365,
      "learning_rate": 1.2477532366609533e-06,
      "loss": 12.2359,
      "step": 19860
    },
    {
      "epoch": 5.76398956219194,
      "grad_norm": 5.963915824890137,
      "learning_rate": 1.1892590578762628e-06,
      "loss": 12.3206,
      "step": 19880
    },
    {
      "epoch": 5.769788344447666,
      "grad_norm": 6.082284450531006,
      "learning_rate": 1.132163629453392e-06,
      "loss": 12.3479,
      "step": 19900
    },
    {
      "epoch": 5.7755871267033925,
      "grad_norm": 6.112476825714111,
      "learning_rate": 1.0764674880622825e-06,
      "loss": 12.2344,
      "step": 19920
    },
    {
      "epoch": 5.781385908959119,
      "grad_norm": 6.463560104370117,
      "learning_rate": 1.0221711572202474e-06,
      "loss": 12.3262,
      "step": 19940
    },
    {
      "epoch": 5.787184691214845,
      "grad_norm": 6.14793062210083,
      "learning_rate": 9.692751472870075e-07,
      "loss": 12.2331,
      "step": 19960
    },
    {
      "epoch": 5.792983473470571,
      "grad_norm": 6.02351713180542,
      "learning_rate": 9.177799554599962e-07,
      "loss": 12.2975,
      "step": 19980
    },
    {
      "epoch": 5.798782255726297,
      "grad_norm": 5.885044574737549,
      "learning_rate": 8.676860657695628e-07,
      "loss": 12.2507,
      "step": 20000
    },
    {
      "epoch": 5.8045810379820235,
      "grad_norm": 6.199808120727539,
      "learning_rate": 8.18993949074509e-07,
      "loss": 12.2687,
      "step": 20020
    },
    {
      "epoch": 5.81037982023775,
      "grad_norm": 6.1531476974487305,
      "learning_rate": 7.717040630576266e-07,
      "loss": 12.3049,
      "step": 20040
    },
    {
      "epoch": 5.816178602493476,
      "grad_norm": 6.045207500457764,
      "learning_rate": 7.258168522214003e-07,
      "loss": 12.3231,
      "step": 20060
    },
    {
      "epoch": 5.821977384749203,
      "grad_norm": 6.275960445404053,
      "learning_rate": 6.813327478838614e-07,
      "loss": 12.3423,
      "step": 20080
    },
    {
      "epoch": 5.827776167004929,
      "grad_norm": 6.223942756652832,
      "learning_rate": 6.382521681744745e-07,
      "loss": 12.2238,
      "step": 20100
    },
    {
      "epoch": 5.8335749492606555,
      "grad_norm": 6.396805286407471,
      "learning_rate": 5.965755180302401e-07,
      "loss": 12.2999,
      "step": 20120
    },
    {
      "epoch": 5.839373731516382,
      "grad_norm": 6.36323881149292,
      "learning_rate": 5.563031891919312e-07,
      "loss": 12.3145,
      "step": 20140
    },
    {
      "epoch": 5.845172513772108,
      "grad_norm": 6.077608585357666,
      "learning_rate": 5.1743556020028e-07,
      "loss": 12.2515,
      "step": 20160
    },
    {
      "epoch": 5.850971296027835,
      "grad_norm": 5.969352722167969,
      "learning_rate": 4.799729963925969e-07,
      "loss": 12.2715,
      "step": 20180
    },
    {
      "epoch": 5.85677007828356,
      "grad_norm": 6.095540523529053,
      "learning_rate": 4.4391584989919016e-07,
      "loss": 12.2557,
      "step": 20200
    },
    {
      "epoch": 5.8625688605392865,
      "grad_norm": 5.553963661193848,
      "learning_rate": 4.092644596401351e-07,
      "loss": 12.2805,
      "step": 20220
    },
    {
      "epoch": 5.868367642795013,
      "grad_norm": 6.188126564025879,
      "learning_rate": 3.760191513220601e-07,
      "loss": 12.2837,
      "step": 20240
    },
    {
      "epoch": 5.874166425050739,
      "grad_norm": 6.286798477172852,
      "learning_rate": 3.4418023743508214e-07,
      "loss": 12.2219,
      "step": 20260
    },
    {
      "epoch": 5.879965207306466,
      "grad_norm": 5.975238800048828,
      "learning_rate": 3.1374801724989295e-07,
      "loss": 12.2857,
      "step": 20280
    },
    {
      "epoch": 5.885763989562192,
      "grad_norm": 5.887335300445557,
      "learning_rate": 2.8472277681492736e-07,
      "loss": 12.2171,
      "step": 20300
    },
    {
      "epoch": 5.891562771817918,
      "grad_norm": 6.20084810256958,
      "learning_rate": 2.571047889536826e-07,
      "loss": 12.3755,
      "step": 20320
    },
    {
      "epoch": 5.897361554073645,
      "grad_norm": 6.107860088348389,
      "learning_rate": 2.3089431326211993e-07,
      "loss": 12.2699,
      "step": 20340
    },
    {
      "epoch": 5.903160336329371,
      "grad_norm": 6.1068220138549805,
      "learning_rate": 2.0609159610628366e-07,
      "loss": 12.3385,
      "step": 20360
    },
    {
      "epoch": 5.908959118585097,
      "grad_norm": 5.834624767303467,
      "learning_rate": 1.826968706199361e-07,
      "loss": 12.3402,
      "step": 20380
    },
    {
      "epoch": 5.914757900840823,
      "grad_norm": 6.030806064605713,
      "learning_rate": 1.6071035670242593e-07,
      "loss": 12.318,
      "step": 20400
    },
    {
      "epoch": 5.9205566830965495,
      "grad_norm": 5.979571342468262,
      "learning_rate": 1.4013226101654006e-07,
      "loss": 12.309,
      "step": 20420
    },
    {
      "epoch": 5.926355465352276,
      "grad_norm": 5.950429439544678,
      "learning_rate": 1.2096277698660506e-07,
      "loss": 12.2654,
      "step": 20440
    },
    {
      "epoch": 5.932154247608002,
      "grad_norm": 6.6687140464782715,
      "learning_rate": 1.0320208479667192e-07,
      "loss": 12.3057,
      "step": 20460
    },
    {
      "epoch": 5.937953029863729,
      "grad_norm": 5.658313274383545,
      "learning_rate": 8.685035138883412e-08,
      "loss": 12.2446,
      "step": 20480
    },
    {
      "epoch": 5.943751812119455,
      "grad_norm": 5.756166934967041,
      "learning_rate": 7.19077304616289e-08,
      "loss": 12.2997,
      "step": 20500
    },
    {
      "epoch": 5.949550594375181,
      "grad_norm": 5.944590091705322,
      "learning_rate": 5.837436246860505e-08,
      "loss": 12.306,
      "step": 20520
    },
    {
      "epoch": 5.955349376630908,
      "grad_norm": 6.155142784118652,
      "learning_rate": 4.625037461699066e-08,
      "loss": 12.3184,
      "step": 20540
    },
    {
      "epoch": 5.961148158886634,
      "grad_norm": 5.972752571105957,
      "learning_rate": 3.553588086651071e-08,
      "loss": 12.3899,
      "step": 20560
    },
    {
      "epoch": 5.966946941142361,
      "grad_norm": 5.790227890014648,
      "learning_rate": 2.62309819283546e-08,
      "loss": 12.3079,
      "step": 20580
    },
    {
      "epoch": 5.972745723398086,
      "grad_norm": 6.12440299987793,
      "learning_rate": 1.8335765264126946e-08,
      "loss": 12.2039,
      "step": 20600
    },
    {
      "epoch": 5.9785445056538125,
      "grad_norm": 6.29734992980957,
      "learning_rate": 1.1850305085131518e-08,
      "loss": 12.2802,
      "step": 20620
    },
    {
      "epoch": 5.984343287909539,
      "grad_norm": 6.0912089347839355,
      "learning_rate": 6.774662351605176e-09,
      "loss": 12.3768,
      "step": 20640
    },
    {
      "epoch": 5.990142070165265,
      "grad_norm": 5.862157344818115,
      "learning_rate": 3.108884772201614e-09,
      "loss": 12.367,
      "step": 20660
    },
    {
      "epoch": 5.995940852420992,
      "grad_norm": 5.92322301864624,
      "learning_rate": 8.530068034917626e-10,
      "loss": 12.2682,
      "step": 20680
    },
    {
      "epoch": 6.0,
      "eval_loss": 12.872751235961914,
      "eval_runtime": 178.9421,
      "eval_samples_per_second": 45.78,
      "eval_steps_per_second": 5.723,
      "step": 20694
    }
  ],
  "logging_steps": 20,
  "max_steps": 20694,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.550166726193971e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
